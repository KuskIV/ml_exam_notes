{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models I\n",
    "The linear models are one of the most commonly used type of models.\n",
    "Here it will be covered how they are used in classification tasks.\n",
    "\n",
    "**Representing the classes**: \n",
    "The number of classes in a problem is defined as $K$. Where the set of classes can be expressed as $C_k$, where $k=1,\\dots,K$.\n",
    "\n",
    "For classification problems where $K=2$, the classes are represented by the labels $0$ and $0$.\n",
    "\n",
    "For classification problem where $K>2$, something called a 1-of-K coding scheme is used. Here a vector of length $K$ is used. If we have a class $C_j$ then all elements of the vector is zero except for the element $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Discriminant functions\n",
    "A discriminant is a function that takes an input vector $x$ and assign's it to on of the $K$ classes. \n",
    "\n",
    "Only linear discriminant functions are covered here.\n",
    "A linear discriminant function is a function where the decision surfaces are hyperplanes.\n",
    "\n",
    "The simplest representation of a linear discriminant function is the obtained by taking the linear function of the input vector so that:\n",
    "$$y(x)=W^TX+w_0$$\n",
    "$W$ is the weight vector\n",
    "\n",
    "$w_0$ is the bias\n",
    "\n",
    "The input vector $x$ is assigned to the class $C_1$ if $y(x)\\geq 0$ and to the class $C_2$ otherwise.\n",
    "\n",
    "This means that the decision boundary is defined by the relation:\n",
    "$$y(x)=0$$\n",
    "\n",
    "Consider two points $X_A$ and $X_B$ both are on the decision boundary.\n",
    "This means that this relation is true:\n",
    "$$y(X_A)=y(X_B)=0$$\n",
    "$$W^T(X_A-X_B)=0$$\n",
    "Meaning that the vector $W$ is orthogonal to the vectors within the decision surface.\n",
    "\n",
    "This means that the vector $W$ determines the orientation of the decision surface.\n",
    "\n",
    "Similarly, the bias $w_0$ determines the position of the decision surface.\n",
    "\n",
    "This can be seen if $x$ is on the decision surface aka $y(x)=0$.\n",
    "Then the normal distance from the origin to the decision surface is:\n",
    "$$\\frac{W^TX}{\\left \\| W \\right \\|} = - \\frac{w_0}{\\left \\| W \\right \\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "*decision regions*: These are the region where different classifications are determined.\n",
    "\n",
    "*decision boundaries/decision surfaces*: These har the linear functions that create the decision regions.\n",
    "\n",
    "*linearly separable*: If the dataset can be separated by a linear decision surfaces.\n",
    "\n",
    "*1-of-K coding scheme* and *one-hot encoding*: These are the two ways to represent the classes in a classification problem."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33c59fad9696fcaf43eb391753f39cb03100e8cd4c5bc920cdc118c109b04286"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('Stonk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
