{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models 2\n",
    "\n",
    "##### Data transformation\n",
    "A mapping is when we transform the data from som representation into another. This can be defined like this:\n",
    "$$\\phi:\\mathbb{R}^D \\rightarrow \\mathbb{R}^{D'}$$\n",
    "Usually the transformed data has more dimensions than the original data.\n",
    "It is also the case that it is typically non-linear. The new space created with the transformed data is called the feature space.\n",
    "\n",
    "support vector machines are a type of linear model that can be used to predict the classes of the data.\n",
    "\n",
    "Kernels can be defined, and SVM classifiers can be learned, for data with very different spaces.\n",
    "\n",
    "The dot product $\\phi(x)\\cdot\\phi(x')$ can be computed without actually constructing the vectors $\\phi(x)$ and $\\phi(x')$.\n",
    "\n",
    "$K(x,x')=(x \\cdot x' + 1 )^2$ is an example of a **kernel function**.\n",
    "Kernel functions can be interpreted as measures of similarity between two vectors.\n",
    "\n",
    "##### Kernel trick\n",
    "The following strategy can be obtain for construction of kernelized SVM classifiers:\n",
    "- **Given:** a classifier problem\n",
    "- **Define:** similarity function between two vectors $\\phi(x)$ and $\\phi(x')$\n",
    "$$K(x,x')$$\n",
    "- **Verify:** That $k(x,x')$ is positive semi-definite\n",
    "- **Learn:** an SVM classifier using the kernel trick \n",
    "\n",
    "##### Constructing Kernels\n",
    "\n",
    "**Basic Kernels**\n",
    "- Polynomial Kernel: $(x*z+1)^p$\n",
    "- Gaussian Kernel: $e^{-\\left \\| x - z \\right \\| / 2\\sigma^2}$\n",
    "- Hyperbolic Tangent Kernel: $tanh(k*X*Z-\\sigma)$\n",
    "\n",
    "**Kernel Building Rules**\n",
    "if $K(x,x')$ is positive semi-definite, then so is:\n",
    "- $q(k(x,x')), where $q()$ is a polynomial with non-negative coefficients\n",
    "- $e^{K(x,x')}$\n",
    "- $K(x,x')K'(x,x')$, where $K'()$ is another positive definite.\n",
    "- $\\frac{K(x,x')}{\\sqrt{K(x,x),K(x',x')}}$ the normalization of $K()$\n",
    "\n",
    "##### Cosine similarity\n",
    "$cos-sim(t_1,t_2)=cosine(\\theta)=\\frac{tf(t_1 \\cdot tf(t_2))}{ \\| tf(t_1) \\|  \\cdot \\| tf(t_2) \\|}$\n",
    "\n",
    "cos-sim is a positive semi-definite kernel: Normalization of plain dot-product.\n",
    "\n",
    "alternatively cos-sim is the dot-product of the feature vectors $tf(t)$ and $\\| tf(t) \\|$\n",
    "\n",
    "cs-sim is extensively used in information retrieval as a measure of similarity between documents.\n",
    "- $t_1$: a short query text\n",
    "- $t_2$: a candidate document\n",
    "\n",
    "\n",
    "##### SVM + Kernel functions:\n",
    "- $+$ Powerful method for classification\n",
    "- $+$ Successful applications\n",
    "- $+$ Wide variety of data types and classification problems reduced to a single type of optimization problem\n",
    "- $-$ Binary classifier only: Generalization to multi-class only via multiple binary classifiers\n",
    "- $-$ Complexity quadratic in number of instances\n",
    "- $-$ To find the \"right\" kernel function may require a lot of engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d081f3a0bf32ee3fdd4d423c9562d27558b8a6ef6eeb0572ab2366ffb3e7c2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
