---
title: 09 - Probabilistic Graphical Models
created: '2022-06-02T07:49:26.203Z'
modified: '2022-06-02T07:50:02.823Z'
---

# 09 - Probabilistic Graphical Models

In probabilistic graphical models, the focus on the Hiddden Markov Model, which enables modeling temporal sequences of events, in terms of the probabilities of certain events happening in certain states.

## Specification of a HMM Model

Formally we characterize a HMM as a 5-tuple of the form $(S, V, A, B, \pi)$ where:

> - $S = \{s_1, ..., s_N\}$ is the set of **states**
> - $V = \{v_1, ..., v_M\}$ is the set of possible **observations** in the states (sensor measurements in TD lingo)
> - $A$ is the **transition matrix**, which is a NxN matrix, where N is the number of states, and  $a_{ij} = P(q_{t+1} = S_i | q_t = S_j)$ is the probability of transitioning from state i at the time t to state j at the time t+1, if one can not transtion from one state to another of course then $a_{ij} = 0$
> - $B$ is the **observation symbol probability distribution** in state j, it is given by $B = \{b_j(k)\}$, where $b_j(k) = P(V_k at t | q_t = S_j)$ that is it denotes the probabilitiy of observing the symmbol $V_k$ at time $t$ in state $S_j$
> - $\pi$ is the **initial state distribution**, it is given by $\pi = \{\pi\}$, where $\pi = P(q_1 = S_i)$ that is it denotes the probabilitiy of starting in state $S_i$

Now what can we use such a HMM to? 

Now given an appropriate 5-tuple $(S, V, A, B, \pi)$  we can use the model as a generator to give an observation sequence $O = O_1, O_2, \ldots, O_T$, where each obseration $O_t$ is a member of the set $V$ and $T$ is the length of the sequence, the process is as follows:

1. We sample a state $q_1 = S_i$ from the initial state distribution $\pi$
2. We sample an observation $O_1 = V_k$ from the observation symbol probability distribution $B$ at state $q_1$
3. We sample a state $q_2 = S_j$ from the transition matrix $A$ at state $q_1$
4. Now we can go back to step one to sample $O_{t+1}$ if $t + 1 \leq T$, otherwise we are done.

The above procedure describes how to use a HMM as a generator of observations. 
Now the HMM is denoted by a 5 tuple but the parameters of the model are the 3-tuple $A,B,\pi$ that is the state transition matrix, the observation symbol probability distribution and the initial state distribution. For convenience denote the model parameters by $\lambda = (A, B, \pi)$

# The 3 Key Basic Problems To Solve With HMMs

Now we can use the HMM to solve the following 3 key basic problems:

- <em>Problem 1:</em> Given a sequence of observations $O = O_1, O_2, \ldots, O_T$ and a HMM model specified by $\lambda = (A,B,\pi)$, how do we effeciently compute $P(O|\lambda)$, that is compute the probability that this sequence was actually generated by the model $\lambda$?
- <em>Problem 2:</em> Given a sequence of observations $O = O_1, O_2, \ldots, O_T$ and a HMM model specified by $\lambda = (A,B,\pi)$, how do we choose a corresponding state sequence $Q = q_1, q_2, \ldots, q_T$ that best explains the observation (most likely sequence of states)?
- <em>Problem 3:</em> How do we adjust the model parameters $\lambda = (A,B,\pi)$ to maximize $P(O|\lambda)$? (that is increase the likelihood of the model parameters $\lambda$)

### Solution to Problem 1
Here we aim to solve the problem of computing the probability of a sequence of observations given a model. That is given $O = O_1, O_2, \ldots, O_T$ and a HMM model specified by $\lambda = (A,B,\pi)$, how do we compute $P(O|\lambda)$.

One naive solution would be to enumerate all possible state transition $Q = q_1, q_2, \ldots, q_T$ and compute the probability of each state sequence $Q$ given the observation sequence $O$. This is a brute force approach and is computationally expensive as there will be $N^T$ such sequences that is $N$ denotes the number of states and $T$ the sequence length, hence assumming all possible such sequences with replacment, and $N^T$ is without even considering cost of computing the probability of $P(O|Q,\lambda) that is considering one such specific state sequence.

However considering the computations will make the following better solution clearer, so we proceed showing the naive apporach:
Now assume we have a fixed state sequence $Q = q_1, q_2, \ldots, q_T$, (we assume independence of the observations), so:

$P(O|Q,\lambda) = \prod_{t=1}^T P(O_t|q_t,\lambda)$

So the probability of observing $O$ given $Q$ and $\lambda$ is just the product of observing each observation $O_t$ given the state $q_t$ and the model parameters $\lambda$. For this we would use $A$ the state transition matrix, and $B$ the observation symbol probability distribution and $pi$ the initial state distribution. Let's unpack this:
- $$

