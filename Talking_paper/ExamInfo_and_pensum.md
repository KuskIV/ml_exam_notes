
Exam information and pensum
Exam Process and Exam Questions for ML 2022




The exam will be held as an oral exam conducted in English. The input material for your preparation is literature for each lecture, lecture slides and exercise and self study material. You are allowed to bring your own notes and/or your laptop to show self study notebooks.

At the exam, you randomly pick 2 topics, one each from Manfred's and Thomas's part.  For each topic you first have approx. 4 minutes to give a short presentation based on your notes and/or self study notebooks. The only material that should be shown on the computer are self study notebooks. Do not prepare extra slides for your presentations. If you do not want to base your presentation on self study material, you can use the whiteboard to support your presentation with a little example.

The presentation is followed by a question and answer period of another 4 minutes. For both your presentation and the question and answer part it is important that underlying models and learning techniques are described in a technically precise manner, and that you can relate the practical experimentation performed in the self studies with scikit-learn and Pytorch to the general theory presented in the lectures. Random example: you should be able to explain the meaning of the parameter setting “activation='logistic',solver='lbfgs'” for neural networks in scikit-learn.

The following is the list of topics, together with a few keywords outlining the scope of each topic:

Manfred’s topics:

1. Linear Models

- Decision Regions
- Overfitting
- Least squares regression (corresponding to sklearn LinearRegression in self study 1)
- Linear discriminant analysis
- Logistic Regression

2. Support Vector Machines

- Maximum margin hypeplanes
- Feature transformations and kernel functions
- The kernel trick
- String kernels

3. Graph data: community detection

- What are communities
- Newman Girvan algorithm
- Modularity
- Node clustering with probabilistic mixture model

4. Graph data: node classification

- Inductive and transductive classification
- Homophily
- Independent classification
- Label propagation
- Node classification with Markov networks

5. Graph data: node embeddings and graph neural networks

- Node embeddings: shallow and functional
- Message passing updates
- Elements of GNN architectures (Skip connections, attention)
- GNNs for node classification, graph classification and link prediction

Thomas’s topics:

1. Neural networks: basics

- Neural network basics
- Loss functions
- Learning neural networks (gradient decent and stochadtic gradient descent)
- Neural network structures


2. Neural networks: learning

- The basis for learning neural networks (loss functions and gradient decent)
- Learning and back propagation (computational graphs, the chain rule)
- Learning in practice (stochastic gradient descent, momentum, regularisation)


3. Probabilistic graphical models

- Probabilistic models (syntax and semantics)
- Inference in probabilistic graphical models
- Maximum likelihood learning
- The EM algorithm 


1. Probabilistic graphical models and structured domains

-  Probabilistic models (syntax and semantics)
-  Temporal models (Hidden Markov models)
-  Plate notation
-  Learning in structured models
-  Bayesian learning


5. Variational inference in probabilistic models

-   Probabilistic models and plate notation
-   Variational inference basics (objective function, Evidence lower bound, mean field assumption)   
-   Black-box variational inference
-   Variational inference and probabilistic programming