- Maximum margin hypeplanes
- Feature transformations and kernel functions
- The kernel trick
- String kernels

**Notebook 3**

In this self study use the MNIST data set to do character recognition using a **Support Vetor Machines (SVM)**.

The objective of **Support Vetor Machines (SVM)** is to find a hyperplane in an N-dimensional space (where N is the number of features) that distinctly classifies the datapoints. The objective is the find a plane that has the maximum margin, i.e the maximum distance between datapoints of both classes and this is done by considering the dot products of support vectors and the samples.

Hyperplanes are decision boundaries that help classify the datapoints. Datpoints falling on either side of the hyperplane can be attributed to different classes.

The dimensionality of the hyperplane depends on the number of features, if the input feature is 2, then the hyperplane is just a line. If the number of features is 3, then the hyperplane becomes a two-dimensional plane.

Support vectors are datapoints closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane.

The margin is the distance between the decision boundary and the closest of the datapoints. Maximizing the margin leads to a particular choice of decision boundary. The location of the boundary is determined by a subset of the datapoints known as support vectors, 

In the first exercise we are only using 3 and 7 from the MNIST dataset. So it is a binary classification. The data data is split into training and test sets. The training set is used to train the model and the test set is used to test the model. 

We test with different kernels. 
First we see the **linear kernel** had an accuracy of 0.98 on the test set. 
**The Polynomial kernel** had an accuracy of 0.99 on the test set. 
**The Gaussian (rbf) kernel** had an accuracy of 0.99 on the test set. 
and the **Sigmoid kernel** had an accuracy of 0.96 on the test set. 

The accuracies and training time are all around the same.

When it is not possible to separate data in a low dimensionality, additional dimensions are added. This enables us to separate the data linearly, but when operations are performed with higher dimensions, this can lead to extremely high and impractical computation costs, especially if many dimensions are added.

This is what the kernel trick solves. The trick is that kernel methods represents the data only through a set of pairwise similarity comparisons between the original data in the low dimensionality, instead of applying the transformation, and representing the data in a higher dimensional feature space.

The benefit is that the objective function we are optimizing to fit the higher dimensional decision boundary only includes the dot product of the transformed vectors. Therefore we can just substitute these dot product terms with the kernel function without using $\phi(x)$, meaning we find the optimal hyperplane using the hither dimensional space, without having to calculate or in reality eve know anything about $\phi(x)$.

Now we try to training on the whole dataset with the gaussian kernel. The accuracy is 0.98. The training time is around 104 seconds. So the trainig time is naturally much larger. Here we have a confusion matrix, where we can see what each class was classified as. We can for example see that 20 9s was classified as 4s and 17 were classified as 7s.

In exercise 2, we create our custom convolution kernels, by taking the dot product between the X and Y. With this kernel an accuracy on the test set of 0.98 is achieved.