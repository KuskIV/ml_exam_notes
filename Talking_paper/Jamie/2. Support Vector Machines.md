- Maximum margin hypeplanes
- Feature transformations and kernel functions
- The kernel trick
- String kernels

**Notebook 3**

The objective of **Support Vetor Machines (SVM)** is to find a hyperplane in an N-dimensional space (where N is the number of features) that distinctly classifies the datapoints. The objective is the find a plane that has the maximum margin, i.e the maximum distance between datapoints of both classes and this is done by considering the dot products of support vectors and the samples.

Hyperplanes are decision boundaries that help classify the datapoints. Datpoints falling on either side of the hyperplane can be attributed to different classes.

The dimensionality of the hyperplane depends on the number of features, if the input feature is 2, then the hyperplane is just a line. If the number of features is 3, then the hyperplane becomes a two-dimensional plane.

Support vectors are datapoints closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane.

The margin is the distance between the decision boundary and the closest of the datapoints. Maximizing the margin leads to a particular choice of decision boundary. The location of the boundary is determined by a subset of the datapoints known as support vectors, 

Here we are only using 3 and 7 from the MNIST dataset. So it is a binary classification. The data data is split into training and test sets. The training set is used to train the model and the test set is used to test the model. We test with different kernels. First we see the linear kernel had an accuracy of 0.98 on the test set. The Polynomial kernel had an accuracy of 0.99 on the test set. The Gaussian (rbf) kernel had an accuracy of 0.99 on the test set. and the Sigmoid kernel had an accuracy of 0.96 on the test set. The accuracies and training time are all around the same.


Now we try to train on the whole dataset with the gaussian kernel. The accuracy is 0.98. The training time is around 104 seconds. So the trainig time is naturally much larger. Here we a confusion matrix, where we can see what each class was classified as. We can for example see that 20 9s was classified as 4s and 17 were classified as 7s.