- Decision Regions
- Overfitting
- Least squares regression (corresponding to sklearn LinearRegression in selfstudy 1)
- Linear discriminant analysis?
- Logistic Regression

** Notebook 01**

The dataset iris was used, which a dataset with 3 classes which are types of iris plants. Three linear models were used in the self study as well as the non-linear classifier **K-nearest neighbor**.

First, **linear Regression** assumes a linear relationship between the input variable and the output variable. When training, a linear equation is fitted to the observed data, where the loss is lowest, for example least square error. This, calculated the best-fitting line by minimizing the sum of squares of the vertical deviations from each point to the line. The optimal score is 0, and it is squared to cancel out between positive and negative values.
We are using **one hot encoding** to encode the categorical variables. 

Now, **Logistic Regression** is used when the target variable is categorical, like whether an email is spam or not. The idea is to find a relationship between featuers and probabilities of particular outcomes. The output will be a number between 0 and 1 using sigmoid. The point is to find a **decision boundry**, which is the threshold used to categorize the datapoints. When training, we find the best weights for the linear model within logistic regression. This is done based on a cost function (LogLoss) where we minimize the function which is optimized using gradient descent.

Now i would like to talk a bit about the decision boundary's and discriminant functions.
Essentially a discriminant function takes some input vector and outputs what class it belongs to.

A linear decision boundary is a function that divides the class with a hyperplane also called decision surface in this context.

Since linear discriminant functions are usually used for binary classification, a work around has to be used. A naive approach to this issue is having multiple linear discriminant functions. But this can results in some classifications becoming ambiguous and essentially meaning nothing.


For the first exercise 

- KNN we see that the decision boundaries are somewhat horizontal lines through the space, which splits the classes into different decision regions. When K is low we can that there can be small regions of a class inside other regions because it only considers few neighbors. This makes it fit more to the training data, but is probably overfitting, and could perform bad on test data. With a high K value it may be computationally expensive and may also be underfitting because it can fit to to outliers.

- Lin. Reg. shows the three linear decision boundaries, which have no overlap in the decision regions, this will result in as we can see these intersecting hyperplanes, as each hyperplane only is fitted on the single class.
it seems not as good as KNN. It misclassifies a bunch of the blue points.

- Log. Reg. shows the three linear decision boundaries, each nicely dividing the data. Log reg is usually used for binary classification problem.

- SVC: generally looks a lot like the log reg. 

For the next exercise we do 70% 30% split, for the train and test set.
This is to compare the accuracy of the KNN and linear regression models.

From this we can see tha the KNN has an accuracy:
KNN Train accuracy 0.9428571428571428
KNN Test accuracy: 0.9333333333333333

and the linear regression has an accuracy:
Linear Regression Train accuracy 0.7904761904761904
Linear Regression Test accuracy: 0.6444444444444445

Here it is clear that the KNN is better than the linear regression.

We can see very clearly from the confusion matrix for linReg. on the test set it has some clear difficulties on class:1 (blue), where it classifies a significant portion as class:2 (green), we can scroll up and we can also see very clearly from the decision boundaries on the train set, why this is happening.

table of confusion matrix for logReg:
 [[ 9  0  0]
 [ 2  5 13]
 [ 0  1 15]]

 For the next exercise we ran the same experiment on all four features in the dataset the results were that both were generally better than with only two.

<!-- **K-nearest neighbor** assumes similar things exists in close proximity. 
- As K is decreased, predictions become less stable. A low k can also lead to overfitting
- As K is increased, predictions becomes more stable due to majority voting/averaging. I can however become too large and lead to underfitting.
Using the euclidean distance, we consider the k nearest neighbors, and that a point will be the same class as the majority of those. To avoid tiebreaks, avoid even k values. -->

**The least squares regression**  is the error function, that tries to minimize the classification error. The least square error has some problems, if the data set contains some outliers. As can be seen here. Least squares regression is not optimal for this kind of dataset.
 

A **linear discriminant function** is a function where the decision surfaces are hyperplanes. 

