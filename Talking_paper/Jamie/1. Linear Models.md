- Decision Regions
- Overfitting
- Least squares regression (corresponding to sklearn LinearRegression in selfstudy 1)
- Linear discriminant analysis?
- Logistic Regression

** Notebook 01**

**linear Regression** assumes a linear relationship between the input variable and the output variable. When training, a linear equation is fitted to the observed data, where the loss is lowest, for example least square error. This, calculated the best-fitting line by minimizing the sum of squares of the vertical deviations from each point to the line. The optimal score is 0, and it is squared to cancel out between positive and negative values.
We are using **one hot encoding** to encode the categorical variables. 

**Logistic Regression** is used when the target variable is categorical, like whether an email is spam or not. The idea is to find a relationship between featuers and probabilities of particular outcomes. The output will be a number between 0 and 1 using sigmoid. The point is to find a **decision boundry**, which is the threshold used to categorize the datapoints. When training, we find the best weights for the linear model within logistic regression. This is done based on a cost function (LogLoss) where we minimize the function which is optimized using gradient descent.

**K-nearest neighbor** assumes similar things exists in close proximity. 
- As K is decreased, predictions become less stable. A low k can also lead to overfitting
- As K is increased, predictions becomes more stable due to majority voting/averaging. I can however become too large and lead to underfitting.
Using the euclidean distance, we consider the k nearest neighbors, and that a point will be the same class as the majority of those. To avoid tiebreaks, avoid even k values.

We can see that the decision boundaries are somewhat horizontal lines through the space which splits the classes into different decision regions. When K is low we can that there can be small regions of a class inside other regions because it only considers few neighbors. This makes it fit more to the training data, but is probably overfitting, and could perform bad on test data. With a high K value it may be computationally expensive and may also be underfitting because it can fit to to outliers.

**The least squares regression**  is the error function, that tries to minimize the classification error. The least square error has some problems, if the data set contains some outliers. As can be seen here. Least squares regression is not optimal for this kind of dataset.
 

A **linear discriminant function** is a function where the decision surfaces are hyperplanes. 

