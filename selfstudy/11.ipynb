{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this self study you will explore Pyro a bit further, bu as opposed to the last self study session where focus was on modeling we will here take a slightly closer lool at (variational) inference. in Pyro \n",
    "\n",
    "Before starting on today's self studies, you should finish the self studies from last tme if you have not already done so. Also, if needed, consider revisiting the Pyro documentation listed under reading material for the last two lectures:\n",
    "* http://pyro.ai/examples/intro_long.html\n",
    "* http://pyro.ai/examples/bayesian_regression.html\n",
    "* http://pyro.ai/examples/svi_part_i.html\n",
    "\n",
    "Afterwards, continue with the notebook below, where we consider (Bayesian) linear regression using Pyro based on the same setup as in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, SGD\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "This data generation is similar to what was done during the lecture; we have one predictor variable 'x' and one response variable 'y', but here collected in a  dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N=10, true_w0= 1., true_w1=.5):\n",
    "    gamma = 4.  # The *precision* in the observation noise\n",
    "    st_dev = 1. / np.sqrt(gamma)  # And corresponding standard deviation\n",
    "    np.random.seed(123)\n",
    "    x = 5 * np.random.rand(N)  # The x-points are sampled uniformly on [0, 5]\n",
    "    y = np.random.normal(loc=true_w0 + true_w1 * x, scale=st_dev)  # And the response is sampled from the Normal\n",
    "    return {\"x\": torch.tensor(x, dtype=torch.float), \"y\": torch.tensor(y, dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for visualizing the data as well as the true and learned functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_plotter(data, true_w0=None, true_w1=None,\n",
    "                 approx_w0=None, approx_w1=None):\n",
    "    \"\"\"\n",
    "    Use to plot data. If y is not none it contains responses, and (x,y) will be scatter-plotted\n",
    "    If neither true_w0 nor true_w1 is None, we will plot the line true_w0 + x * true_w1 in red.\n",
    "    If neither approx_w0 nor approx_w1 is None, we plot the line approx_w0 + x * approx_w1 in green.\n",
    "    \"\"\"\n",
    "    if data is not None:\n",
    "        plt.plot(data[\"x\"].numpy(), data[\"y\"].numpy(), \"bo\")\n",
    "\n",
    "    # Plot true line if given\n",
    "    if true_w0 is not None and true_w1 is not None:\n",
    "        plt.plot(data[\"x\"].numpy(), true_w0 + true_w1 * data[\"x\"].numpy(), \"r-\")\n",
    "\n",
    "    # Plot approximation if given\n",
    "    if approx_w0 is not None and approx_w1 is not None:\n",
    "        plt.plot(data[\"x\"].numpy(), approx_w0+ approx_w1* data[\"x\"].numpy(), \"g-\", alpha=.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a data set with 50 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w0= 1.\n",
    "true_w1=.5\n",
    "data = generate_data(N=10, true_w0=true_w0, true_w1=true_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data together with the regression line around which the data has been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plotter(data, true_w0=true_w0, true_w1=true_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the Pyro model and guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify a (Bayesian) linear regression model in Pyro. The 'data' argument is a dictionary covering the data of the predictor and response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_model(data):\n",
    "\n",
    "    w0 = pyro.sample(\"w0\", dist.Normal(0.0, 1000.0))\n",
    "    w1 = pyro.sample(\"w1\", dist.Normal(0.0, 1000.0))\n",
    "\n",
    "    with pyro.plate(\"data_plate\"):\n",
    "        pyro.sample(\"y\", dist.Normal(data[\"x\"] * w1 + w0, 1.0), obs=data[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify the variational distribution, which is called a guide in Pyro. We make the mean field assumption and assume that the variational distribution factorizes wrt. to 'w0' and 'w1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_guide(data):\n",
    "    w0_mean = pyro.param(\"w0_mean\", torch.tensor(0.0))\n",
    "    w0_scale = pyro.param(\"w0_scale\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"w0\", dist.Normal(w0_mean, w0_scale))\n",
    "    \n",
    "    w1_mean = pyro.param(\"w1_mean\", torch.tensor(0.0))\n",
    "    w1_scale = pyro.param(\"w1_scale\", torch.tensor(1.0), constraint=constraints.positive)\n",
    "    pyro.sample(\"w1\", dist.Normal(w1_mean, w1_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "In this function the actual learning is taking place. Notice that the structure is similar to what we saw in the example notebooks during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(N=10, data=None):\n",
    "    if data is None:\n",
    "        data = generate_data(N=N)\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    elbo = pyro.infer.Trace_ELBO()\n",
    "    svi = pyro.infer.SVI(model=lin_reg_model,\n",
    "                         guide=lin_reg_guide,\n",
    "                         optim=SGD({\"lr\": 0.0001}),\n",
    "                         loss=elbo)\n",
    "\n",
    "    num_steps = 5000\n",
    "    for step in range(num_steps):\n",
    "        loss = svi.step(data)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            w0_mean = pyro.param(\"w0_mean\").detach().item()\n",
    "            w0_scale = pyro.param(\"w0_scale\").detach().item()\n",
    "            w1_mean = pyro.param(\"w1_mean\").detach().item()\n",
    "            w1_scale = pyro.param(\"w1_scale\").detach().item()\n",
    "            print(f\"Loss (iter: {step}): {loss}\")\n",
    "            print(f\"w0: {w0_mean} +/- {w0_scale}\\t \\t w1: {w1_mean} +/- {w1_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the results\n",
    "\n",
    "Here we sample weights from the posterior distributions over 'w0' and 'w1'. The distribution of the generated weights (and the corresponding models) illustrates how confident we are in the model, an insight you cannot get when only having point estimates of the model parameters as found with, e.g., maximum likelihood learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    w0_mean = pyro.param(\"w0_mean\").detach().item()\n",
    "    w0_scale = pyro.param(\"w0_scale\").detach().item()\n",
    "    w1_mean = pyro.param(\"w1_mean\").detach().item()\n",
    "    w1_scale = pyro.param(\"w1_scale\").detach().item()\n",
    "    w0_sample = pyro.sample(\"w0_sample\", dist.Normal(w0_mean, w0_scale)).numpy()\n",
    "    w1_sample = pyro.sample(\"w1_sample\", dist.Normal(w1_mean, w1_scale)).numpy()\n",
    "    data_plotter(data, approx_w0=w0_sample, approx_w1=w1_sample)\n",
    "data_plotter(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "* Generate data sets of varying sizes and characteristics (by changing the parameters in the 'generate_data' function) and investigate and compare the resulting models.\n",
    "* Analyze how learning is affected by changing the learning rate and the initial values of the parameters specified in the guide function.\n",
    "* Experiment with different types of prior knowledge in the model specification (e.g. change the mean and scale of the distributions over the weights). For instance, we may (mostly likely erroneously considering the data) have a prior expectation that 'w0' is around 5.0, and we can encode the strength of this belief through the scale of the corresponding distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-2021",
   "language": "python",
   "name": "ml-2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
