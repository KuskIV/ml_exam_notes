{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d20dd14",
   "metadata": {},
   "source": [
    "# Self study 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "# NOTE: the following is only needed to visualize graphs. There can be some version \n",
    "# compatibility issues with networkx and pyplot that lead to errors in the nx graph \n",
    "# drawing functions. If you encounter such problems, do not spend much time on trying\n",
    "# to fix them, as this is not central for the self study.\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a utility for plotting networkx graphs that we already used in self study 4\n",
    "\n",
    "def get_att_array(G,att_name):\n",
    "    ret_array=np.zeros(nx.number_of_nodes(G))\n",
    "    for i,n in enumerate(G.nodes()):\n",
    "        ret_array[i]=G.nodes[n][att_name]\n",
    "    return(ret_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4139046f",
   "metadata": {},
   "source": [
    "### GNN design\n",
    "\n",
    "We define a message passing layer, and a message passing graph neural network consisting of two message passing layers, and a linear layer for binary classification. The details of the Pytorch Geometric graph neural network functions are somewhat involved, and it is not the goal of this self study to investigate them in depth. The following explanations should convey a rough idea of how models are constructed and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f328bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MPLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        # the arguments aggr='add',flow=\"source_to_target\" specify that neighbor nodes are aggregated by \n",
    "        # summation, and only along \"incoming\" edges (this makes no difference\n",
    "        # when the graph is undirected):\n",
    "        super().__init__(aggr='add',flow=\"source_to_target\")\n",
    "        # The two linear functions defined by matrices U and W as on lecture slide 13\n",
    "        self.U=torch.nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.W=torch.nn.Linear(in_channels, out_channels, bias=False)\n",
    "        \n",
    "    def message(self,x_j):\n",
    "        # This function defines a transformation of the current node representation\n",
    "        # before they are aggregated. Note that applying the linear function U to the sum \n",
    "        # of neighbor vectors h_j (as on the lecture slides) is the same as applying U to each vector\n",
    "        # h_j first, and then doing the summation (which is what happens here in the code)\n",
    "        #\n",
    "        # the _j notation is a Pytorch geometric speciality to refer to the set of neighbor vectors\n",
    "        return self.U(x_j)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Key Pytorch fundtion to define the computation of a layer.\n",
    "        # x has shape [number of nodes, dimension of input feature vectors]\n",
    "        # edge_index has shape [2, number of edges]\n",
    "        \n",
    "        # linear transformation of input features \n",
    "        Wx = self.W(x)\n",
    "        \n",
    "        # The propagate method calls the message function defined above\n",
    "        return Wx + self.propagate(edge_index, x=x)\n",
    "\n",
    "#   The update function can implement additional layer computations. We don't need it here    \n",
    "#     def update(self,x):\n",
    "#         return x\n",
    "    \n",
    "    \n",
    "    def init(self):\n",
    "        # Random initialization of the U and W matrices by uniformly sampled values from [-1,1]\n",
    "        self.U.weight.data.uniform_(-1,1)\n",
    "        self.W.weight.data.uniform_(-1,1)\n",
    "\n",
    "# Now we define a GNN for binary node classification with two message passing layers, followed by\n",
    "# linear classification layer. For simplicity, here both message passing layers construct \n",
    "# feature vectors of the same dimension 'hiddim'.\n",
    "class MPGNN(nn.Module):\n",
    "    def __init__(self,indim,hiddim):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            MPLayer(indim,hiddim),\n",
    "            nn.ReLU(),\n",
    "            MPLayer(hiddim,hiddim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddim,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x,edges):\n",
    "        h1=self.layers[0].forward(x,edges)\n",
    "        h2=self.layers[1].forward(h1)\n",
    "        h3=self.layers[2].forward(h2,edges)\n",
    "        h4=self.layers[3].forward(h3)\n",
    "        h5=self.layers[4].forward(h4)\n",
    "        return h5\n",
    "    \n",
    "    def init(self):\n",
    "        self.layers[0].init()\n",
    "        self.layers[2].init()\n",
    "        self.layers[4].weight.data.uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea0afa",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "We next create a data graph as on slides 16,17. The following code contains a duplicate construction: we create the graph simultaneously in the input format required by Pytorch geometric, and in the form of a networkx graph that we use for plotting. The Pytorch representation consists of:\n",
    "\n",
    "edge_index: a (2 x #edges) tensor containing the edges\n",
    "\n",
    "x: a (#nodes x #attributes) matrix containing the node attributes (initial representation)\n",
    "\n",
    "y: a vector of length #nodes containing the class labels \n",
    "\n",
    "It would be nicer to specify the graph only in one format, and write a function that then casts it into the other format, but there was no time for that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326094e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating regular example graphs as on slides 16,17\n",
    "\n",
    "size = 8 # the widht/height of the grid\n",
    "numnodes = size**2\n",
    "\n",
    "edge_index=[]\n",
    "G=nx.Graph()\n",
    "\n",
    "# edges\n",
    "for i in range(size):\n",
    "    for j in range(size-1):\n",
    "        # \"horizontal edges in both directions\"\n",
    "        edge_index.append([size*i+j,size*i+j+1])\n",
    "        edge_index.append([size*i+j+1,size*i+j])\n",
    "        # \"vertical\" edges in both directions\"\n",
    "        edge_index.append([size*j+i,size*(j+1)+i])\n",
    "        edge_index.append([size*(j+1)+i,size*j+i])\n",
    "        # same for the networkx graph. Here we need to add the edges only\n",
    "        # in one direction, because nx.Graph() is an undirected graph by default\n",
    "        G.add_edge(size*i+j,size*i+j+1)\n",
    "        G.add_edge(size*j+i,size*(j+1)+i)\n",
    "\n",
    "\n",
    "edge_index = torch.tensor(edge_index)\n",
    "edge_index = edge_index.transpose(0,1)\n",
    "\n",
    "# node attributes: one-hot encoding of the identifiers is just the\n",
    "# identity matrix constructed by torch.eye\n",
    "x = torch.eye(numnodes)\n",
    "\n",
    "# class label: we here manually label all nodes with distance <=2 to node '26' as belonging to class '1' \n",
    "y = torch.zeros(numnodes,dtype=torch.int64)\n",
    "y[[24,25,26,27,28,17,18,19,33,34,35,42,10]]=1\n",
    "for i in range (numnodes):\n",
    "    G.add_node(i,classlabel=y[i])\n",
    "    \n",
    "# defining train/test nodes:\n",
    "testnodes=(0,7,19,25,30,48,52,61) # a random selection of a set of test nodes\n",
    "\n",
    "# Train and test nodes are defined in Pytorch Geometric by boolean \"masking\" vectors\n",
    "train_mask=torch.ones(numnodes,dtype=torch.bool)\n",
    "test_mask=torch.zeros(numnodes,dtype=torch.bool)\n",
    "\n",
    "for i in range(numnodes):\n",
    "    G.add_node(i,testnode=False)\n",
    "for i in testnodes:\n",
    "    train_mask[i]=False\n",
    "    test_mask[i]=True\n",
    "    G.add_node(i,testnode=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432aa72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph with class labels:\")\n",
    "nx.draw_kamada_kawai(G,with_labels=True,node_color=get_att_array(G,'classlabel'))\n",
    "plt.show()\n",
    "print(\"Graph with train/test split:\")\n",
    "nx.draw_kamada_kawai(G,with_labels=True,node_color=get_att_array(G,'testnode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e110a",
   "metadata": {},
   "source": [
    "### Now training and evaluation: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c92f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the GNN model\n",
    "\n",
    "hiddim=8\n",
    "gnn=MPGNN(numnodes,hiddim)\n",
    "\n",
    "# Defining optimizer and loss function\n",
    "optimizer = torch.optim.Adam(gnn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "\n",
    "# Initializing the model for training. This is not yet doing the actual training!\n",
    "gnn.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e35379",
   "metadata": {},
   "source": [
    "Now comes the actual training. At the end we plot the curves of accuracies on training and test nodes, and visualize the predictions on the graph. Since we here have only a tiny dataset, we do not divide the data further into batches. Each 'epoch' of training takes all the labeled training nodes in one batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a30ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "accuracies_train=[]\n",
    "accuracies_test=[]\n",
    "\n",
    "gnn.init()\n",
    "\n",
    "for epoch in range(100):\n",
    "    t=0 # count of true predictions\n",
    "    f=0 # count of false predictions\n",
    "    epochloss=0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "    # forward propagation\n",
    "    out = gnn(x,edge_index)\n",
    "\n",
    "    # The following code is only to record the accuracies on train and test nodes\n",
    "    # during the iterations. It is not part of the actual learning process\n",
    "    pred = torch.argmax(out,1)   \n",
    "    t_train=len(np.where(y[train_mask]==pred[train_mask])[0])\n",
    "    f_train= len(y[train_mask])-t_train\n",
    "    accuracies_train.append(t_train/len(y[train_mask]))\n",
    "    \n",
    "    t_test=len(np.where(y[test_mask]==pred[test_mask])[0])\n",
    "    f_test= len(y[test_mask])-t_test\n",
    "    accuracies_test.append(t_test/len(y[test_mask]))\n",
    "    \n",
    "\n",
    "    # Now back to the key learning operations: calculate loss and backpropagate\n",
    "    l = loss(out[train_mask],y[train_mask])\n",
    "    epochloss+=l.detach().item()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(epochloss)\n",
    "    \n",
    "     \n",
    "\n",
    "print(\"final train accuracy: {}\".format(accuracies_train[-1]))    \n",
    "print(\"final test accuracy: {}\".format(accuracies_test[-1]))\n",
    "\n",
    "fig,axes = plt.subplots(1,1)\n",
    "rightax=axes.twinx()\n",
    "axes.plot(losses,c='r',label='loss')\n",
    "rightax.plot(accuracies_train,c='b',label='train acc.')\n",
    "rightax.plot(accuracies_test,c='g',label='test acc.')\n",
    "# Stuff for constructing a joint legend for curves associated with left and\n",
    "# right y-axes\n",
    "lines, labels = axes.get_legend_handles_labels()\n",
    "linesr, labelsr = rightax.get_legend_handles_labels()\n",
    "rightax.legend(lines + linesr, labels + labelsr)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Adding the predicted class as another node attribute:\n",
    "for i in range(numnodes):\n",
    "    G.add_node(i,pred=pred[i])\n",
    "\n",
    "print(\"Predicted classes:\")\n",
    "nx.draw_kamada_kawai(G,with_labels=True,node_color=get_att_array(G,'pred'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc882c2",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "\n",
    "<ul>\n",
    "<li>Conduct experiments with the given model and graph: what train/test accuracies do you get on average when running the experiment multiple times? How does this change when you increase or decrease the number of training epochs? </li>\n",
    "<li> What is the minimum value of hiddim for which the model can accurately represent the target classification? What happens in practice when you increase/decrease the hiddim value?    \n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b55411",
   "metadata": {},
   "source": [
    "## Task 2: \n",
    "\n",
    "Add to your training graph a color attribute as on slide 17. Train the model on this graph and evaluate\n",
    "\n",
    "<ul>\n",
    "    <li> on the test nodes of the training graph (as in Task 1)\n",
    "    </li>\n",
    "    <li> on nodes in new completely unlabeled graphs. For this, construct new graphs where the nodes have a yellow/blue color attribute. The new graphs can resemble the original training graph to a greater or lesser extent. E.g., they can again be regular rectangular grids, or they can have very different structures. Do the predictions on the new graphs match the \"definition\" of the class label that a node is red if it is at a distance at most 2 from a blue node?\n",
    "    </li> \n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0e22f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
