{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this self study you should experiment with convolutional neural networks using PyTorch. In the last self study session we only made limited use of PyTorch (only using it for calculating gradients), but in this self study we will take advantage of much more of its functionality.\n",
    "\n",
    "In particular, we will work with the _torch.nn_ module provided by PyTorch. A short introduction to this module and how to define neural networks in PyTorch can be found at\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "For this self study you may either go through these tutorials before working on the notebook or consult themt when needed as you move forward in the notebook. The former tutorial is part of a general tutorial package to PyTorch, which can be found at (this also includes a nice introduction to tensors in PyTorch)\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import relevant modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last time we will be working with the MNIST data set: The MNIST database consists of grey scale images of handwritten digits. Each image is of size $28\\times 28$; see figure below for an illustration. The data set is divided into a training set consisting of $60000$ images and a test set with $10000$ images; in both\n",
    "data sets the images are labeled with the correct digits. If interested you can find more information about the MNIST data set at http://yann.lecun.com/exdb/mnist/, including accuracy results for various machine learning methods.\n",
    "\n",
    "![MNIST DATA](images/MNIST-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this self study, we will be a bit more careful with our data. Specifically, we will divide the data into a training, validation, and test, and use the training and validation set for model learning (in the previous self study we did not have a validation set). \n",
    "\n",
    "The data set is created by setting aside a randomly chosen subset of the data, where the splitting point is found using the help function *split_indicies* below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 48000\n",
      "Number of validation examples: 12000\n"
     ]
    }
   ],
   "source": [
    "def split_indicies(n, val_pct):\n",
    "    # Size of validation set\n",
    "    n_val = int(n*val_pct)\n",
    "    # Random permutation\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Return first indexes for the validation set\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "# Load the data\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "\n",
    "# Get the indicies for the training data and test data (the validation set will consists of 20% of the data)\n",
    "train_idxs, val_idxs = split_indicies(len(train_dataset), 0.2)\n",
    "\n",
    "# Define samplers (used by Dataloader) to the two sets of indicies\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "\n",
    "# Specify data loaders for our training and test set (same functionality as in the previous self study)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, sampler=val_sampler)\n",
    "\n",
    "print(f\"Number of training examples: {len(train_idxs)}\")\n",
    "print(f\"Number of validation examples: {len(val_idxs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set is loaded in the usual fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the model\n",
    "\n",
    "When using the _torch.nn_ for specifying our model we subclass the _nn.Module_. The model thus holds all the parameters of the model (see the _init_ function) as well as a specification of the forward step. We don't have to keep track of the backward pass, as PyTorch handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a convolution operator with 1 input channel, 15 output channels and a kernel size of 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 15, 5)\n",
    "        # Since we are not doing padding (see Lecture 2, Slide 38) the width of the following layer is reduced; for\n",
    "        # each channel the resulting dimension is 24x24. We feed the resulting representation through a linear \n",
    "        # layer, giving 10 values as output - one for each digit.\n",
    "        self.fc = nn.Linear(15 * 24 * 24, 10)\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # Reshape the input tensor; '-1' indicates that PyTorch will fill-in this \n",
    "        # dimension, whereas the '1' indicates that we only have one color channel. \n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        # Apply convolution and pass the result through a ReLU function\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        # Reshape the representation\n",
    "        xb = xb.view(-1, 15*24*24)\n",
    "        # Apply the linear layer\n",
    "        xb = self.fc(xb)\n",
    "        # and set the result as the output. Note that we don't take a softmax as this is handled internally in the \n",
    "        # loss function defined below.\n",
    "        self.out = xb\n",
    "\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and evaluating the model\n",
    "\n",
    "For learning the model, we will use the following function which performs one iteration over the training data. The function also takes an _epoch_ argument, but this is only used for reporting on the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, epoch, optimizer):\n",
    "    losses = []\n",
    "    # Tell PyTorch that this function is part of the training\n",
    "    model.train()\n",
    "\n",
    "    # As optimizer we use stochastic gradient descent as defined by PyTorch. PyTorch also includes a variety \n",
    "    # of other optimizers \n",
    "    # learning_rate = 0.01\n",
    "    opt = optimizer #torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Iterate over the training set, one batch at the time, as in the previous self sudy\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get the prediction\n",
    "        y_pred = model(data)\n",
    "        \n",
    "        # Remember to zero the gradients so that they don't accumulate\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Calculate the loss and and the gradients  \n",
    "        loss = loss_fn(y_pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the parameters by taking one 'step' with the optimizer\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # For every 10th batch we output a bit of info\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.sampler),\n",
    "                       100. * batch_idx * len(data) / len(train_loader.sampler), loss.item()))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we also want to validate our model. To do this we define the function below, which takes a data_loader (either the validation or test set) and reports the model's accuracy and loss on that data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data_loader, loss_fn):\n",
    "    # Tell PyTorch that we are performing evaluation\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            losses.append(loss_fn(output, target).item())\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    print('\\nTest/validation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.sampler),\n",
    "        100. * correct / len(data_loader.sampler)))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A couple of helper functions\n",
    "\n",
    "Learning a deep neural network can be time consuming, and it might therefore be nice to be able to save and load previously learned models (see also https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_name, model):\n",
    "    torch.save(model, file_name)\n",
    "\n",
    "def load_model(file_name):\n",
    "    model = torch.load(file_name)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping things up\n",
    "\n",
    "Finally, we will do the actual learning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of passes that will be made over the training set\n",
    "num_epochs = 2\n",
    "# torch.nn defines several useful loss-functions, which we will take advantage of here (see Lecture 1, Slide 11, Log-loss).\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "MNIST_CNN(\n",
      "  (conv1): Conv2d(1, 15, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc): Linear(in_features=8640, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model class\n",
    "model = MNIST_CNN()\n",
    "# and get some information about the structure\n",
    "lr = 0.001\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print('Model structure:')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over the data set\n",
    "\n",
    "We iterate over the data set for *num_epochs* number of iterations. At each iteration we also calculate the loss/accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 0.026007\n",
      "Train Epoch: 0 [640/48000 (1%)]\tLoss: 0.021594\n",
      "Train Epoch: 0 [1280/48000 (3%)]\tLoss: 0.052532\n",
      "Train Epoch: 0 [1920/48000 (4%)]\tLoss: 0.041245\n",
      "Train Epoch: 0 [2560/48000 (5%)]\tLoss: 0.034001\n",
      "Train Epoch: 0 [3200/48000 (7%)]\tLoss: 0.042681\n",
      "Train Epoch: 0 [3840/48000 (8%)]\tLoss: 0.010947\n",
      "Train Epoch: 0 [4480/48000 (9%)]\tLoss: 0.024882\n",
      "Train Epoch: 0 [5120/48000 (11%)]\tLoss: 0.016502\n",
      "Train Epoch: 0 [5760/48000 (12%)]\tLoss: 0.070414\n",
      "Train Epoch: 0 [6400/48000 (13%)]\tLoss: 0.060689\n",
      "Train Epoch: 0 [7040/48000 (15%)]\tLoss: 0.042067\n",
      "Train Epoch: 0 [7680/48000 (16%)]\tLoss: 0.019967\n",
      "Train Epoch: 0 [8320/48000 (17%)]\tLoss: 0.019863\n",
      "Train Epoch: 0 [8960/48000 (19%)]\tLoss: 0.113716\n",
      "Train Epoch: 0 [9600/48000 (20%)]\tLoss: 0.013850\n",
      "Train Epoch: 0 [10240/48000 (21%)]\tLoss: 0.058371\n",
      "Train Epoch: 0 [10880/48000 (23%)]\tLoss: 0.010325\n",
      "Train Epoch: 0 [11520/48000 (24%)]\tLoss: 0.032446\n",
      "Train Epoch: 0 [12160/48000 (25%)]\tLoss: 0.068544\n",
      "Train Epoch: 0 [12800/48000 (27%)]\tLoss: 0.037647\n",
      "Train Epoch: 0 [13440/48000 (28%)]\tLoss: 0.006153\n",
      "Train Epoch: 0 [14080/48000 (29%)]\tLoss: 0.008063\n",
      "Train Epoch: 0 [14720/48000 (31%)]\tLoss: 0.008842\n",
      "Train Epoch: 0 [15360/48000 (32%)]\tLoss: 0.117748\n",
      "Train Epoch: 0 [16000/48000 (33%)]\tLoss: 0.015889\n",
      "Train Epoch: 0 [16640/48000 (35%)]\tLoss: 0.028343\n",
      "Train Epoch: 0 [17280/48000 (36%)]\tLoss: 0.008960\n",
      "Train Epoch: 0 [17920/48000 (37%)]\tLoss: 0.046999\n",
      "Train Epoch: 0 [18560/48000 (39%)]\tLoss: 0.040133\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 0.061111\n",
      "Train Epoch: 0 [19840/48000 (41%)]\tLoss: 0.104354\n",
      "Train Epoch: 0 [20480/48000 (43%)]\tLoss: 0.009777\n",
      "Train Epoch: 0 [21120/48000 (44%)]\tLoss: 0.039768\n",
      "Train Epoch: 0 [21760/48000 (45%)]\tLoss: 0.077248\n",
      "Train Epoch: 0 [22400/48000 (47%)]\tLoss: 0.023414\n",
      "Train Epoch: 0 [23040/48000 (48%)]\tLoss: 0.107832\n",
      "Train Epoch: 0 [23680/48000 (49%)]\tLoss: 0.017790\n",
      "Train Epoch: 0 [24320/48000 (51%)]\tLoss: 0.055088\n",
      "Train Epoch: 0 [24960/48000 (52%)]\tLoss: 0.068408\n",
      "Train Epoch: 0 [25600/48000 (53%)]\tLoss: 0.044061\n",
      "Train Epoch: 0 [26240/48000 (55%)]\tLoss: 0.016884\n",
      "Train Epoch: 0 [26880/48000 (56%)]\tLoss: 0.043868\n",
      "Train Epoch: 0 [27520/48000 (57%)]\tLoss: 0.019439\n",
      "Train Epoch: 0 [28160/48000 (59%)]\tLoss: 0.010253\n",
      "Train Epoch: 0 [28800/48000 (60%)]\tLoss: 0.145300\n",
      "Train Epoch: 0 [29440/48000 (61%)]\tLoss: 0.054314\n",
      "Train Epoch: 0 [30080/48000 (63%)]\tLoss: 0.022406\n",
      "Train Epoch: 0 [30720/48000 (64%)]\tLoss: 0.151946\n",
      "Train Epoch: 0 [31360/48000 (65%)]\tLoss: 0.034579\n",
      "Train Epoch: 0 [32000/48000 (67%)]\tLoss: 0.054829\n",
      "Train Epoch: 0 [32640/48000 (68%)]\tLoss: 0.024917\n",
      "Train Epoch: 0 [33280/48000 (69%)]\tLoss: 0.020649\n",
      "Train Epoch: 0 [33920/48000 (71%)]\tLoss: 0.016901\n",
      "Train Epoch: 0 [34560/48000 (72%)]\tLoss: 0.015350\n",
      "Train Epoch: 0 [35200/48000 (73%)]\tLoss: 0.021645\n",
      "Train Epoch: 0 [35840/48000 (75%)]\tLoss: 0.034019\n",
      "Train Epoch: 0 [36480/48000 (76%)]\tLoss: 0.013817\n",
      "Train Epoch: 0 [37120/48000 (77%)]\tLoss: 0.019330\n",
      "Train Epoch: 0 [37760/48000 (79%)]\tLoss: 0.066137\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 0.012672\n",
      "Train Epoch: 0 [39040/48000 (81%)]\tLoss: 0.056458\n",
      "Train Epoch: 0 [39680/48000 (83%)]\tLoss: 0.034387\n",
      "Train Epoch: 0 [40320/48000 (84%)]\tLoss: 0.065693\n",
      "Train Epoch: 0 [40960/48000 (85%)]\tLoss: 0.027605\n",
      "Train Epoch: 0 [41600/48000 (87%)]\tLoss: 0.075159\n",
      "Train Epoch: 0 [42240/48000 (88%)]\tLoss: 0.020928\n",
      "Train Epoch: 0 [42880/48000 (89%)]\tLoss: 0.017004\n",
      "Train Epoch: 0 [43520/48000 (91%)]\tLoss: 0.048627\n",
      "Train Epoch: 0 [44160/48000 (92%)]\tLoss: 0.017660\n",
      "Train Epoch: 0 [44800/48000 (93%)]\tLoss: 0.041201\n",
      "Train Epoch: 0 [45440/48000 (95%)]\tLoss: 0.076081\n",
      "Train Epoch: 0 [46080/48000 (96%)]\tLoss: 0.053056\n",
      "Train Epoch: 0 [46720/48000 (97%)]\tLoss: 0.018807\n",
      "Train Epoch: 0 [47360/48000 (99%)]\tLoss: 0.073678\n",
      "\n",
      "Test/validation set: Average loss: 0.0002, Accuracy: 11746/12000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.027399\n",
      "Train Epoch: 1 [640/48000 (1%)]\tLoss: 0.018182\n",
      "Train Epoch: 1 [1280/48000 (3%)]\tLoss: 0.076454\n",
      "Train Epoch: 1 [1920/48000 (4%)]\tLoss: 0.015944\n",
      "Train Epoch: 1 [2560/48000 (5%)]\tLoss: 0.022990\n",
      "Train Epoch: 1 [3200/48000 (7%)]\tLoss: 0.047930\n",
      "Train Epoch: 1 [3840/48000 (8%)]\tLoss: 0.035082\n",
      "Train Epoch: 1 [4480/48000 (9%)]\tLoss: 0.027933\n",
      "Train Epoch: 1 [5120/48000 (11%)]\tLoss: 0.030370\n",
      "Train Epoch: 1 [5760/48000 (12%)]\tLoss: 0.041136\n",
      "Train Epoch: 1 [6400/48000 (13%)]\tLoss: 0.165348\n",
      "Train Epoch: 1 [7040/48000 (15%)]\tLoss: 0.043471\n",
      "Train Epoch: 1 [7680/48000 (16%)]\tLoss: 0.032296\n",
      "Train Epoch: 1 [8320/48000 (17%)]\tLoss: 0.017241\n",
      "Train Epoch: 1 [8960/48000 (19%)]\tLoss: 0.062739\n",
      "Train Epoch: 1 [9600/48000 (20%)]\tLoss: 0.049169\n",
      "Train Epoch: 1 [10240/48000 (21%)]\tLoss: 0.022342\n",
      "Train Epoch: 1 [10880/48000 (23%)]\tLoss: 0.014608\n",
      "Train Epoch: 1 [11520/48000 (24%)]\tLoss: 0.057566\n",
      "Train Epoch: 1 [12160/48000 (25%)]\tLoss: 0.062181\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 0.011145\n",
      "Train Epoch: 1 [13440/48000 (28%)]\tLoss: 0.034344\n",
      "Train Epoch: 1 [14080/48000 (29%)]\tLoss: 0.020515\n",
      "Train Epoch: 1 [14720/48000 (31%)]\tLoss: 0.031041\n",
      "Train Epoch: 1 [15360/48000 (32%)]\tLoss: 0.059630\n",
      "Train Epoch: 1 [16000/48000 (33%)]\tLoss: 0.035135\n",
      "Train Epoch: 1 [16640/48000 (35%)]\tLoss: 0.054044\n",
      "Train Epoch: 1 [17280/48000 (36%)]\tLoss: 0.051694\n",
      "Train Epoch: 1 [17920/48000 (37%)]\tLoss: 0.082107\n",
      "Train Epoch: 1 [18560/48000 (39%)]\tLoss: 0.007695\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 0.063653\n",
      "Train Epoch: 1 [19840/48000 (41%)]\tLoss: 0.019418\n",
      "Train Epoch: 1 [20480/48000 (43%)]\tLoss: 0.012944\n",
      "Train Epoch: 1 [21120/48000 (44%)]\tLoss: 0.137901\n",
      "Train Epoch: 1 [21760/48000 (45%)]\tLoss: 0.019632\n",
      "Train Epoch: 1 [22400/48000 (47%)]\tLoss: 0.011812\n",
      "Train Epoch: 1 [23040/48000 (48%)]\tLoss: 0.088353\n",
      "Train Epoch: 1 [23680/48000 (49%)]\tLoss: 0.018535\n",
      "Train Epoch: 1 [24320/48000 (51%)]\tLoss: 0.044100\n",
      "Train Epoch: 1 [24960/48000 (52%)]\tLoss: 0.005644\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.129301\n",
      "Train Epoch: 1 [26240/48000 (55%)]\tLoss: 0.038497\n",
      "Train Epoch: 1 [26880/48000 (56%)]\tLoss: 0.094851\n",
      "Train Epoch: 1 [27520/48000 (57%)]\tLoss: 0.011240\n",
      "Train Epoch: 1 [28160/48000 (59%)]\tLoss: 0.068844\n",
      "Train Epoch: 1 [28800/48000 (60%)]\tLoss: 0.080464\n",
      "Train Epoch: 1 [29440/48000 (61%)]\tLoss: 0.012843\n",
      "Train Epoch: 1 [30080/48000 (63%)]\tLoss: 0.031114\n",
      "Train Epoch: 1 [30720/48000 (64%)]\tLoss: 0.069502\n",
      "Train Epoch: 1 [31360/48000 (65%)]\tLoss: 0.047775\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.044658\n",
      "Train Epoch: 1 [32640/48000 (68%)]\tLoss: 0.021396\n",
      "Train Epoch: 1 [33280/48000 (69%)]\tLoss: 0.016978\n",
      "Train Epoch: 1 [33920/48000 (71%)]\tLoss: 0.035059\n",
      "Train Epoch: 1 [34560/48000 (72%)]\tLoss: 0.022046\n",
      "Train Epoch: 1 [35200/48000 (73%)]\tLoss: 0.019487\n",
      "Train Epoch: 1 [35840/48000 (75%)]\tLoss: 0.118160\n",
      "Train Epoch: 1 [36480/48000 (76%)]\tLoss: 0.040342\n",
      "Train Epoch: 1 [37120/48000 (77%)]\tLoss: 0.015564\n",
      "Train Epoch: 1 [37760/48000 (79%)]\tLoss: 0.042785\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 0.021544\n",
      "Train Epoch: 1 [39040/48000 (81%)]\tLoss: 0.124001\n",
      "Train Epoch: 1 [39680/48000 (83%)]\tLoss: 0.006874\n",
      "Train Epoch: 1 [40320/48000 (84%)]\tLoss: 0.013938\n",
      "Train Epoch: 1 [40960/48000 (85%)]\tLoss: 0.037038\n",
      "Train Epoch: 1 [41600/48000 (87%)]\tLoss: 0.011258\n",
      "Train Epoch: 1 [42240/48000 (88%)]\tLoss: 0.048755\n",
      "Train Epoch: 1 [42880/48000 (89%)]\tLoss: 0.007947\n",
      "Train Epoch: 1 [43520/48000 (91%)]\tLoss: 0.005369\n",
      "Train Epoch: 1 [44160/48000 (92%)]\tLoss: 0.017559\n",
      "Train Epoch: 1 [44800/48000 (93%)]\tLoss: 0.031030\n",
      "Train Epoch: 1 [45440/48000 (95%)]\tLoss: 0.069272\n",
      "Train Epoch: 1 [46080/48000 (96%)]\tLoss: 0.212729\n",
      "Train Epoch: 1 [46720/48000 (97%)]\tLoss: 0.166064\n",
      "Train Epoch: 1 [47360/48000 (99%)]\tLoss: 0.077532\n",
      "\n",
      "Test/validation set: Average loss: 0.0002, Accuracy: 11753/12000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    train_losses.extend(train(model, train_loader, loss_fn, i, optimizer))\n",
    "    # Evaluate the model on the test set\n",
    "    test_losses.extend(test_model(model, val_loader, loss_fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning we evaluate the model on the _test set_ and save the resulting structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.030578596517443657,\n",
       " 0.02941614855080843,\n",
       " 0.030515209461251896,\n",
       " 0.03012579120695591,\n",
       " 0.0400108203291893,\n",
       " 0.03934938584764799,\n",
       " 0.04414943924971989,\n",
       " 0.045835102908313274,\n",
       " 0.04174212138685915,\n",
       " 0.042880857922136785,\n",
       " 0.039880626665597614,\n",
       " 0.04093884187750518,\n",
       " 0.03951198700815439,\n",
       " 0.04565300825717194,\n",
       " 0.04438287559896707,\n",
       " 0.04522982978960499,\n",
       " 0.04541637085597305,\n",
       " 0.045437384552011885,\n",
       " 0.0462404487066363,\n",
       " 0.04491249811835587,\n",
       " 0.04419171273530949,\n",
       " 0.042644134341654455,\n",
       " 0.04328655309813178,\n",
       " 0.04405432027609398,\n",
       " 0.04492973316460848,\n",
       " 0.045708483120856375,\n",
       " 0.04830278970163177,\n",
       " 0.04733036614821425,\n",
       " 0.047394613221544646,\n",
       " 0.04895336177820961,\n",
       " 0.04970504965392813,\n",
       " 0.048992630356224254,\n",
       " 0.04802137277455944,\n",
       " 0.047431183841956014,\n",
       " 0.04714020083525351,\n",
       " 0.04678032917177512,\n",
       " 0.04614332386267346,\n",
       " 0.04545639394047229,\n",
       " 0.04478350637528377,\n",
       " 0.044576360401697455,\n",
       " 0.044359094145276196,\n",
       " 0.044907007565987964,\n",
       " 0.04547312927194113,\n",
       " 0.045310407390140674,\n",
       " 0.04454064807958073,\n",
       " 0.04550006200114022,\n",
       " 0.044997673798748784,\n",
       " 0.04790184685649971,\n",
       " 0.04721259223107172,\n",
       " 0.04660027660429478,\n",
       " 0.04707622856778257,\n",
       " 0.04670685866417793,\n",
       " 0.04606021591022892,\n",
       " 0.04573971227984185,\n",
       " 0.047586230815134266,\n",
       " 0.047045146863508434,\n",
       " 0.04684411239271101,\n",
       " 0.047293650195131015,\n",
       " 0.046664120061165194,\n",
       " 0.04627284814293186,\n",
       " 0.04631036865051653,\n",
       " 0.04588665699045504,\n",
       " 0.04565176184451769,\n",
       " 0.04698367981472984,\n",
       " 0.0467060373379634,\n",
       " 0.046631324528293175,\n",
       " 0.04625493015593557,\n",
       " 0.04828555460142739,\n",
       " 0.04767083174859484,\n",
       " 0.04875357228863452,\n",
       " 0.04894620959948696,\n",
       " 0.048843231860599995,\n",
       " 0.048842772873348164,\n",
       " 0.04954319551108858,\n",
       " 0.049161531211187444,\n",
       " 0.04886568982864877,\n",
       " 0.049530960117503034,\n",
       " 0.049305197693264254,\n",
       " 0.049216601131130244,\n",
       " 0.048835475608939305,\n",
       " 0.04856287605807553,\n",
       " 0.048711991761016045,\n",
       " 0.0493533917904708,\n",
       " 0.04923602837204401,\n",
       " 0.04887798361151534,\n",
       " 0.048442745456731946,\n",
       " 0.04803307031698782,\n",
       " 0.047801243210084395,\n",
       " 0.048277002942403044,\n",
       " 0.047931919872967736,\n",
       " 0.04770240016220199,\n",
       " 0.047804600280790546,\n",
       " 0.04780385275221159,\n",
       " 0.04801790820969705,\n",
       " 0.049870812348825365,\n",
       " 0.04991325329077275,\n",
       " 0.049732812752313525,\n",
       " 0.049544537824825666,\n",
       " 0.04952932747946394,\n",
       " 0.04970582526642829,\n",
       " 0.049347702695038353,\n",
       " 0.04903017762847537,\n",
       " 0.0488022331157428,\n",
       " 0.04873609249569619,\n",
       " 0.04898875107811321,\n",
       " 0.049231710795418555,\n",
       " 0.04932686930661586,\n",
       " 0.0491787833377236,\n",
       " 0.04902260135773809,\n",
       " 0.04887327125549994,\n",
       " 0.04881481507893752,\n",
       " 0.0497721415387267,\n",
       " 0.050112291482334904,\n",
       " 0.04990815124964636,\n",
       " 0.049684077132817195,\n",
       " 0.04947337311098416,\n",
       " 0.049290870774027884,\n",
       " 0.049046666191701414,\n",
       " 0.04945997130360548,\n",
       " 0.04940285332268104,\n",
       " 0.04919964418754228,\n",
       " 0.0488656776476285,\n",
       " 0.049664951327097855,\n",
       " 0.04938804770203968,\n",
       " 0.04929146576300263,\n",
       " 0.04977848156473585,\n",
       " 0.04980786620294602,\n",
       " 0.049496256029669894,\n",
       " 0.049412475953008544,\n",
       " 0.049251038878439714,\n",
       " 0.04934270743579241,\n",
       " 0.04954659409298928,\n",
       " 0.049389836816047146,\n",
       " 0.04918239269502906,\n",
       " 0.0490537373383564,\n",
       " 0.04883890479515471,\n",
       " 0.04864883880695178,\n",
       " 0.048619695638805846,\n",
       " 0.04919854340833618,\n",
       " 0.04997499030016895,\n",
       " 0.05024195043370128,\n",
       " 0.050099593124837735,\n",
       " 0.049871385771572796,\n",
       " 0.04986404743653515,\n",
       " 0.04971191377924948,\n",
       " 0.04972665020494326,\n",
       " 0.04953593064771116,\n",
       " 0.04981993836967426,\n",
       " 0.049774582763421255,\n",
       " 0.04970368719038864,\n",
       " 0.049602687392778545,\n",
       " 0.05053208556062983,\n",
       " 0.05045114962530194,\n",
       " 0.05019265737735316,\n",
       " 0.050293299270373196,\n",
       " 0.050171059308549724,\n",
       " 0.05022413778611144,\n",
       " 0.050193674853543126,\n",
       " 0.05007971386548202,\n",
       " 0.049937202097498815,\n",
       " 0.0505920142912948,\n",
       " 0.0507474203772245,\n",
       " 0.0506054179920055,\n",
       " 0.050633081305976504,\n",
       " 0.05115688280907996,\n",
       " 0.05120463941122843,\n",
       " 0.05151315160865288,\n",
       " 0.0513702742783131,\n",
       " 0.051252931645746236,\n",
       " 0.05152432184721179,\n",
       " 0.05147505702167662,\n",
       " 0.05170724933241429,\n",
       " 0.05156998512082103,\n",
       " 0.05144634087377324,\n",
       " 0.051537599842995406,\n",
       " 0.051567041360116986,\n",
       " 0.05132358536745111,\n",
       " 0.051211729821006066,\n",
       " 0.051120474713453865,\n",
       " 0.0508628266165033,\n",
       " 0.05077942391054818,\n",
       " 0.05060214760385781,\n",
       " 0.05066286471463699,\n",
       " 0.05062951633716812,\n",
       " 0.050426463201339984,\n",
       " 0.050377088531311,\n",
       " 0.050258622821022,\n",
       " 0.05024326522586907,\n",
       " 0.05014055099328438,\n",
       " 0.05015454219016982,\n",
       " 0.05002266973383842,\n",
       " 0.04988490290755484,\n",
       " 0.050034019123724245,\n",
       " 0.049916759798226434,\n",
       " 0.05044803379867704,\n",
       " 0.05027256363632195,\n",
       " 0.05012937330021547,\n",
       " 0.0502257836027767,\n",
       " 0.0501910372187931,\n",
       " 0.04999929910292849,\n",
       " 0.04993366933571388,\n",
       " 0.04988713219236251,\n",
       " 0.04979078527402334,\n",
       " 0.04987874941891242,\n",
       " 0.049739375744560145,\n",
       " 0.049611465722883066,\n",
       " 0.050061721937361976,\n",
       " 0.04999425510043064,\n",
       " 0.05025371949023536,\n",
       " 0.0500794012387771,\n",
       " 0.04991224438384599,\n",
       " 0.049855835773138645,\n",
       " 0.04999675010790078,\n",
       " 0.0500209529636097,\n",
       " 0.050012488266843,\n",
       " 0.05027408589376137,\n",
       " 0.050614405391214215,\n",
       " 0.05097301466894601,\n",
       " 0.05081406156808886,\n",
       " 0.05069444518409331,\n",
       " 0.05069343907741737,\n",
       " 0.050732368487982196,\n",
       " 0.051017798819566654,\n",
       " 0.05090681139388055,\n",
       " 0.050927450576176245,\n",
       " 0.05078699397112746,\n",
       " 0.05071196368883163,\n",
       " 0.05069144086691698,\n",
       " 0.05055504970000366,\n",
       " 0.050395951709588584,\n",
       " 0.05029279540909769,\n",
       " 0.050108057737816125,\n",
       " 0.050051038183447756,\n",
       " 0.05017151226862692,\n",
       " 0.05003914838022691,\n",
       " 0.05002980840479229,\n",
       " 0.05012505099310968,\n",
       " 0.05004659429889079,\n",
       " 0.04999726521490321,\n",
       " 0.050207514410916095,\n",
       " 0.050235648993093567,\n",
       " 0.05042307426929782,\n",
       " 0.050279788628849104,\n",
       " 0.050281079778004986,\n",
       " 0.05030607824612941,\n",
       " 0.05027922873036951,\n",
       " 0.050478689283931906,\n",
       " 0.05052457179220754,\n",
       " 0.05046802650142207,\n",
       " 0.05081455539353192,\n",
       " 0.05077992560920784,\n",
       " 0.050733032682540986,\n",
       " 0.05058433968581171,\n",
       " 0.050589820140914066,\n",
       " 0.05092217468419204,\n",
       " 0.05110030553441902,\n",
       " 0.05097683571126849,\n",
       " 0.05098822737264252,\n",
       " 0.05117094290755603,\n",
       " 0.05109983861553841,\n",
       " 0.050987419181671066,\n",
       " 0.05089999672615517,\n",
       " 0.05093171050729577,\n",
       " 0.05098023231557542,\n",
       " 0.05090581971343677,\n",
       " 0.05096894197158964,\n",
       " 0.050819589545399176,\n",
       " 0.050693752176350734,\n",
       " 0.050632621970055386,\n",
       " 0.0505467447159053,\n",
       " 0.05043090427830486,\n",
       " 0.05038584937403143,\n",
       " 0.05025455157269106,\n",
       " 0.050426473455286044,\n",
       " 0.05033929279074073,\n",
       " 0.05040425468620885,\n",
       " 0.050282181205315385,\n",
       " 0.05017322799184179,\n",
       " 0.050672018815392766,\n",
       " 0.05055391577770933,\n",
       " 0.05047901776317653,\n",
       " 0.05057031860251429,\n",
       " 0.05051417270504175,\n",
       " 0.05038935577162397,\n",
       " 0.05094854148235499,\n",
       " 0.051016269123077496,\n",
       " 0.05093521321231324,\n",
       " 0.05093918627082732,\n",
       " 0.05083499639536734,\n",
       " 0.05088984983780517,\n",
       " 0.050763920322380626,\n",
       " 0.05095305953538428,\n",
       " 0.05095182735720846,\n",
       " 0.05087877007448399,\n",
       " 0.051258625199039606,\n",
       " 0.05116428580367938,\n",
       " 0.05123823544524786,\n",
       " 0.05124182152349327,\n",
       " 0.051127785569162774,\n",
       " 0.05102048982710888,\n",
       " 0.05094898401286169,\n",
       " 0.05098527919852625,\n",
       " 0.05084824610790453,\n",
       " 0.0507520709545229,\n",
       " 0.050722866323700204,\n",
       " 0.05075230640724447,\n",
       " 0.0508256380246036,\n",
       " 0.050843729239936866,\n",
       " 0.05069881049060175,\n",
       " 0.050958203387657,\n",
       " 0.05093414986422255,\n",
       " 0.051108096041179336,\n",
       " 0.05103580934105637,\n",
       " 0.051035451217833316,\n",
       " 0.05101683662080812,\n",
       " 0.05098288320741746,\n",
       " 0.051053208749952846,\n",
       " 0.050945760636830574,\n",
       " 0.050957286025347846,\n",
       " 0.050957537117938045,\n",
       " 0.051132807689443276,\n",
       " 0.05100075265548846,\n",
       " 0.0508649445199422,\n",
       " 0.050789827972443566,\n",
       " 0.05081723349312177,\n",
       " 0.051201084762971645,\n",
       " 0.05109974255198641,\n",
       " 0.05104798484854854,\n",
       " 0.05096741725257555,\n",
       " 0.050888114984175475,\n",
       " 0.050902243655771465,\n",
       " 0.050854652803054595,\n",
       " 0.0507306726145986,\n",
       " 0.05080014440191988,\n",
       " 0.05105218397759235,\n",
       " 0.05122444612096019,\n",
       " 0.05144735885292115,\n",
       " 0.0513623548677114,\n",
       " 0.051320124997216165,\n",
       " 0.051260376324438875,\n",
       " 0.05143703583387598,\n",
       " 0.051493044569543754,\n",
       " 0.05169720910543864,\n",
       " 0.05161456382281125,\n",
       " 0.05152295488000348,\n",
       " 0.051537047256042676,\n",
       " 0.051575403394827066,\n",
       " 0.05163702846440519,\n",
       " 0.051715442929470265,\n",
       " 0.05178656795461263,\n",
       " 0.05168385054661423,\n",
       " 0.051652891966196796,\n",
       " 0.05172385670318566,\n",
       " 0.05160100809903552,\n",
       " 0.05177132379473515,\n",
       " 0.051797446776495386,\n",
       " 0.051682296422954535,\n",
       " 0.05190351672272859,\n",
       " 0.0518037104657475,\n",
       " 0.05180446670338926,\n",
       " 0.05177132569903565,\n",
       " 0.05189057537586073,\n",
       " 0.05183779997571523,\n",
       " 0.051754584057502204,\n",
       " 0.051892683637162595,\n",
       " 0.05194860867026155,\n",
       " 0.05192403984827063,\n",
       " 0.051819862104425934,\n",
       " 0.05181923316322207,\n",
       " 0.051727440469973796,\n",
       " 0.05169257208343465,\n",
       " 0.05163127407231318,\n",
       " 0.051803709165100756,\n",
       " 0.051708589646626606,\n",
       " 0.05164827338854472,\n",
       " 0.051556618497806024,\n",
       " 0.051481923379932855,\n",
       " 0.05137749510033736,\n",
       " 0.051328977368437834,\n",
       " 0.05142085340462233,\n",
       " 0.051489321109191015,\n",
       " 0.05151067448887214,\n",
       " 0.051466487532885206,\n",
       " 0.051540017583950735,\n",
       " 0.05146742494559133,\n",
       " 0.05136494048258261,\n",
       " 0.05138559076569262,\n",
       " 0.05132604314772815,\n",
       " 0.05123869219799658,\n",
       " 0.05125471069119297,\n",
       " 0.051225515184900185,\n",
       " 0.051193913884403905,\n",
       " 0.051236996500641487,\n",
       " 0.051202547138433925,\n",
       " 0.05116621687112353,\n",
       " 0.05111414828394173,\n",
       " 0.05129281758220505,\n",
       " 0.05137715062089077,\n",
       " 0.05129308155726146,\n",
       " 0.05127432547742501,\n",
       " 0.051241071976052405,\n",
       " 0.051198583524859516,\n",
       " 0.051214107289152315,\n",
       " 0.05113403009721032,\n",
       " 0.05108279367609892,\n",
       " 0.05099301380816425,\n",
       " 0.050966902947195185,\n",
       " 0.050935593070736265,\n",
       " 0.050954852382991744,\n",
       " 0.050891578717656975,\n",
       " 0.05086796261469886,\n",
       " 0.05075427595394871,\n",
       " 0.05067128941899314,\n",
       " 0.05069625822507781,\n",
       " 0.050798467056354484,\n",
       " 0.050785306477337144,\n",
       " 0.0507367699210503,\n",
       " 0.050704475976838664,\n",
       " 0.050721152037657743,\n",
       " 0.05067727299118858,\n",
       " 0.05060131337621964,\n",
       " 0.05056700726940133,\n",
       " 0.050505362058071594,\n",
       " 0.05050481616339277,\n",
       " 0.05052212335059748,\n",
       " 0.05051102753654297,\n",
       " 0.0505652438698716,\n",
       " 0.05046027538988948,\n",
       " 0.050728098553386114,\n",
       " 0.05082751121480278,\n",
       " 0.05098001216916398,\n",
       " 0.05116442822596851,\n",
       " 0.05126958346106198,\n",
       " 0.051228017375637105,\n",
       " 0.0512599761683451,\n",
       " 0.051260705376172076,\n",
       " 0.051371360795457316,\n",
       " 0.05138120526919932,\n",
       " 0.051394636117999604,\n",
       " 0.05188925992379981,\n",
       " 0.051909089938456575,\n",
       " 0.05184614276239539,\n",
       " 0.05174930086023012,\n",
       " 0.05196444457082229,\n",
       " 0.052053381788303675,\n",
       " 0.05226102434704289,\n",
       " 0.052286451595983316,\n",
       " 0.05220459528728887,\n",
       " 0.052138647121607526,\n",
       " 0.05218855525366962,\n",
       " 0.05219281190890662,\n",
       " 0.05238523313145162,\n",
       " 0.052427602577214405,\n",
       " 0.05267215010070033,\n",
       " 0.052660383352661856,\n",
       " 0.052898968474963975,\n",
       " 0.053277339422525084,\n",
       " 0.05332328725960024,\n",
       " 0.05327327175609654,\n",
       " 0.053245544857750446,\n",
       " 0.053217074790375546,\n",
       " 0.05312480862066956,\n",
       " 0.05325110014387231,\n",
       " 0.05335080515139672,\n",
       " 0.05356621310335173,\n",
       " 0.05352539557847694,\n",
       " 0.053444384327018736,\n",
       " 0.05341479259754062,\n",
       " 0.053367634416063396,\n",
       " 0.053330602396794775,\n",
       " 0.053240098851098185,\n",
       " 0.053239053479293204,\n",
       " 0.053261176524506626,\n",
       " 0.053184648084686086,\n",
       " 0.05319854896025438,\n",
       " 0.053142642474169804,\n",
       " 0.05320443245494422,\n",
       " 0.05314437653109525,\n",
       " 0.05305915323644099,\n",
       " 0.0531111046448738,\n",
       " 0.0531062705998005,\n",
       " 0.053055193473264206,\n",
       " 0.053120457975089734,\n",
       " 0.053035276925599215,\n",
       " 0.05301684769369738,\n",
       " 0.05296517612217109,\n",
       " 0.05291445820938482,\n",
       " 0.05283480432155527,\n",
       " 0.052761141053669476,\n",
       " 0.05271664438864254,\n",
       " 0.052739499655576866,\n",
       " 0.052715358096250615,\n",
       " 0.052636140835628245,\n",
       " 0.052613234663050366,\n",
       " 0.05266034119489669,\n",
       " 0.052641195961416906,\n",
       " 0.052853953425905535,\n",
       " 0.052803104125284946,\n",
       " 0.052708697923363926,\n",
       " 0.05267787968181074,\n",
       " 0.05265606058475203,\n",
       " 0.0525883092893755,\n",
       " 0.05254255929335981,\n",
       " 0.052460968267128226,\n",
       " 0.05238022822886705,\n",
       " 0.05232058461615632,\n",
       " 0.05234963463571591,\n",
       " 0.05237451778526792,\n",
       " 0.052300843240030856,\n",
       " 0.052337691469081474,\n",
       " 0.052400975475368435,\n",
       " 0.05242864771935274,\n",
       " 0.05240281901120791,\n",
       " 0.05239954436784936,\n",
       " 0.05249091179553166,\n",
       " 0.05246534749013625,\n",
       " 0.05241797176303209,\n",
       " 0.05242842599807099,\n",
       " 0.052366363857235754,\n",
       " 0.052395851444453,\n",
       " 0.052335744123055014,\n",
       " 0.05226006268941123,\n",
       " 0.05220184470553585,\n",
       " 0.05237313661056052,\n",
       " 0.05235187102996168,\n",
       " 0.052290968820027754,\n",
       " 0.052282403656184334,\n",
       " 0.052346693778721,\n",
       " 0.05233062256923357,\n",
       " 0.052339705863510666,\n",
       " 0.052269610115055085,\n",
       " 0.05217854555388142,\n",
       " 0.05223678569632668,\n",
       " 0.052238339446624565,\n",
       " 0.05216985506954316,\n",
       " 0.052087243547343384,\n",
       " 0.052013293770618835,\n",
       " 0.05203963598915026,\n",
       " 0.051974133204786256,\n",
       " 0.051986644611935376,\n",
       " 0.05207790737513749,\n",
       " 0.05201875694900256,\n",
       " 0.05196428886043597,\n",
       " 0.05188153593746178,\n",
       " 0.05187027340908663,\n",
       " 0.05185051288305621,\n",
       " 0.051786979878324695,\n",
       " 0.05173660281460977,\n",
       " 0.05179368483007823,\n",
       " 0.05171877828172662,\n",
       " 0.05173049217762726,\n",
       " 0.05167799158885643,\n",
       " 0.051676750129213384,\n",
       " 0.0517079374258699,\n",
       " 0.05185254922321251,\n",
       " 0.05178487251626609,\n",
       " 0.05188591667589743,\n",
       " 0.051922462002698025,\n",
       " 0.0519582981906818,\n",
       " 0.05197751457204244,\n",
       " 0.05195984512288621,\n",
       " 0.05195607340089261,\n",
       " 0.051880185244204624,\n",
       " 0.05200477474342688,\n",
       " 0.051969960643456575,\n",
       " 0.051988472071124156,\n",
       " 0.05193259146038239,\n",
       " 0.052042998709637196,\n",
       " 0.05217113437029568,\n",
       " 0.05210290679773479,\n",
       " 0.05204017796176902,\n",
       " 0.05199326641395219,\n",
       " 0.05202096089701222,\n",
       " 0.05199047727899736,\n",
       " 0.05208921175450087,\n",
       " 0.05209339704500356,\n",
       " 0.052015441741013135,\n",
       " 0.052051004852171484,\n",
       " 0.05197203018910063,\n",
       " 0.05203399389564734,\n",
       " 0.05214551084621997,\n",
       " 0.05214847832809353,\n",
       " 0.05212702922759447,\n",
       " 0.05210320582479391,\n",
       " 0.052071033085449636,\n",
       " 0.052107358839261146,\n",
       " 0.052035203558952886,\n",
       " 0.05197491497751706,\n",
       " 0.051939733677459896,\n",
       " 0.051859238825075454,\n",
       " 0.05188319518730397,\n",
       " 0.05185418132915967,\n",
       " 0.05182823365943704,\n",
       " 0.05191017886049895,\n",
       " 0.05186862148454084,\n",
       " 0.0518338830865007,\n",
       " 0.05176361629714444,\n",
       " 0.051783678692650284,\n",
       " 0.051725425778457484,\n",
       " 0.051723033662419766,\n",
       " 0.05170409121130672,\n",
       " 0.05164582150573672,\n",
       " 0.051654871597200336,\n",
       " 0.05158957739943478,\n",
       " 0.051628974741247814,\n",
       " 0.051569824204580374,\n",
       " 0.051579966032616734,\n",
       " 0.05156117809651493,\n",
       " 0.05151902365135809,\n",
       " 0.051455404295692925,\n",
       " 0.05144317491809226,\n",
       " 0.0515009467315014,\n",
       " 0.05146182473174632,\n",
       " 0.05143297206479995,\n",
       " 0.051437023882489136,\n",
       " 0.05144822512308813,\n",
       " 0.05147239335886591,\n",
       " 0.05147644640609598,\n",
       " 0.051632216986365054,\n",
       " 0.05173175978519383,\n",
       " 0.05180277137367742,\n",
       " 0.0518349153910527,\n",
       " 0.05203275207391257,\n",
       " 0.05197531689415113,\n",
       " 0.0520251196987927,\n",
       " 0.05204157170708687,\n",
       " 0.052051715874387935,\n",
       " 0.052010917876680995,\n",
       " 0.052014622518227145,\n",
       " 0.05202664141691039,\n",
       " 0.0521282505623335,\n",
       " 0.05216383743114605,\n",
       " 0.05212460916509961,\n",
       " 0.05205900917396725,\n",
       " 0.05201897259405631,\n",
       " 0.05208889919453236,\n",
       " 0.05211604601415561,\n",
       " 0.05209921260924899,\n",
       " 0.05214272569121679,\n",
       " 0.05210401214353624,\n",
       " 0.052107452072766895,\n",
       " 0.05214424115007981,\n",
       " 0.05227308572540827,\n",
       " 0.052239961131846084,\n",
       " 0.05230479061011543,\n",
       " 0.052431567957705524,\n",
       " 0.05237231434465399,\n",
       " 0.05231663891772544,\n",
       " 0.05225940341365241,\n",
       " 0.05234837756635478,\n",
       " 0.052302067512116995,\n",
       " 0.05236482760723269,\n",
       " 0.05231263220900667,\n",
       " 0.052551067914865024,\n",
       " 0.0525190795416311,\n",
       " 0.0525766117604756,\n",
       " 0.052635761983607594,\n",
       " 0.05259410951974431,\n",
       " 0.052613670969514775,\n",
       " 0.05261218493000012,\n",
       " 0.05255235566682945,\n",
       " 0.05250213097913185,\n",
       " 0.05251356598262311,\n",
       " 0.05258212850852701,\n",
       " 0.05266641252569126,\n",
       " 0.05261168194243925,\n",
       " 0.05258026596708879,\n",
       " 0.05254819735948799,\n",
       " 0.05249974779960086,\n",
       " 0.052514648367303296,\n",
       " 0.052701826419003985,\n",
       " 0.052782270155841526,\n",
       " 0.052898975293308406,\n",
       " 0.052934864305860696,\n",
       " 0.052916124553454146,\n",
       " 0.052894436315490075,\n",
       " 0.05283108805175568,\n",
       " 0.052794216583265885,\n",
       " 0.052780163215832655,\n",
       " 0.0527634875390076,\n",
       " 0.052702681428825324,\n",
       " 0.052678226709742745,\n",
       " 0.05270900725186591,\n",
       " 0.05267057073376093,\n",
       " 0.052602661585258524,\n",
       " 0.05261274605226478,\n",
       " 0.052562681184688466,\n",
       " 0.052567124625987394,\n",
       " 0.052499098115950736,\n",
       " 0.05252497401830835,\n",
       " 0.05262370794059981,\n",
       " 0.052686524393731414,\n",
       " 0.052712811734312325,\n",
       " 0.05277912411503279,\n",
       " 0.05295461243045309,\n",
       " 0.0529413969495505,\n",
       " 0.05291069831787321,\n",
       " 0.053026557517742316,\n",
       " 0.053013481806633254,\n",
       " 0.053008211057500115,\n",
       " 0.05299267036170724,\n",
       " 0.05296005047738361,\n",
       " 0.05302506680156543,\n",
       " 0.05300918218497166,\n",
       " 0.052959253495651566,\n",
       " 0.05290114429710755,\n",
       " 0.05286937794428124,\n",
       " 0.052841604039425526,\n",
       " 0.05283573859309507,\n",
       " 0.0527865746825881,\n",
       " 0.05275919021378401,\n",
       " 0.05281611754012221,\n",
       " 0.052761894003533844,\n",
       " 0.05299110124892026,\n",
       " 0.05295281769134677,\n",
       " 0.052921259473338096,\n",
       " 0.052871535148346895,\n",
       " 0.052930060763062496,\n",
       " 0.052887293668813555,\n",
       " 0.05284205678164856,\n",
       " 0.05282010289358593,\n",
       " 0.052848149745639963,\n",
       " 0.05279266323448567,\n",
       " 0.05273305876527846,\n",
       " 0.0527401071600616,\n",
       " 0.0526834709458207,\n",
       " 0.05265958922398898,\n",
       " 0.052670044518091236,\n",
       " 0.05263472499009043,\n",
       " 0.05263562068049732,\n",
       " 0.05257865327579847,\n",
       " 0.05265535892285101,\n",
       " 0.05270533721784757,\n",
       " 0.05273960670519732,\n",
       " 0.05270485966624756,\n",
       " 0.05266257290887828,\n",
       " 0.05270625769604235,\n",
       " 0.05266768954439356,\n",
       " 0.052691701327668594,\n",
       " 0.052731874551800256,\n",
       " 0.05282123535356199,\n",
       " 0.05287560263982499,\n",
       " 0.05287059488446824,\n",
       " 0.052928948381249744,\n",
       " 0.05293395779964408,\n",
       " 0.05289720797264604,\n",
       " 0.05285019825188969,\n",
       " 0.05281160353934442,\n",
       " 0.05279542035168318,\n",
       " 0.05274262515145044,\n",
       " 0.04733984172344208,\n",
       " 0.05858389288187027,\n",
       " 0.043557013695438705,\n",
       " 0.03805885510519147,\n",
       " 0.03643028140068054,\n",
       " 0.034822266238431134,\n",
       " 0.0373118453259979,\n",
       " 0.035569075495004654,\n",
       " 0.03768881493144565,\n",
       " 0.0378301601856947,\n",
       " 0.03900070590051738,\n",
       " 0.038352353808780514,\n",
       " 0.03677648678421974,\n",
       " 0.038034264530454366,\n",
       " 0.04770006239414215,\n",
       " 0.05224837688729167,\n",
       " 0.050026589949779654,\n",
       " 0.04777089051074452,\n",
       " 0.04552939311160069,\n",
       " 0.04460340777877718,\n",
       " 0.043779164840955107,\n",
       " 0.04384803816422143,\n",
       " 0.045762974384200315,\n",
       " 0.04496476979693398,\n",
       " 0.04428032921627164,\n",
       " 0.043865476394645296,\n",
       " 0.04329503067182722,\n",
       " 0.04298445025259363,\n",
       " 0.04333522686220963,\n",
       " 0.04249394661746919,\n",
       " 0.042116417174017234,\n",
       " 0.041675435451907106,\n",
       " 0.041532198771495714,\n",
       " 0.04165798268171356,\n",
       " 0.040885907132178545,\n",
       " 0.040212027922583125,\n",
       " 0.03991106151275941,\n",
       " 0.03995462407750126,\n",
       " 0.03912759245110628,\n",
       " 0.04019460638519377,\n",
       " 0.03954066783644077,\n",
       " 0.04268447072466924,\n",
       " 0.04350141082825356,\n",
       " 0.04288141054778614,\n",
       " 0.04280096530500385,\n",
       " 0.04253638521565691,\n",
       " 0.04323946337829879,\n",
       " 0.04346314757519091,\n",
       " 0.042814349318493386,\n",
       " 0.042553445380181076,\n",
       " 0.043023666019971464,\n",
       " 0.0428312942922975,\n",
       " 0.04261048617860619,\n",
       " 0.042049157398718374,\n",
       " 0.04215674197131937,\n",
       " 0.04256896435150078,\n",
       " 0.04228826403095011,\n",
       " 0.04193923474642737,\n",
       " 0.042144343698933974,\n",
       " 0.042045732215046885,\n",
       " 0.04245827701248107,\n",
       " 0.04222082319639383,\n",
       " 0.04238052018696353,\n",
       " 0.04247921580099501,\n",
       " 0.04202435429279621,\n",
       " 0.04163382938978347,\n",
       " 0.04117663161578908,\n",
       " 0.04070671588894637,\n",
       " 0.04120393158139094,\n",
       " 0.04115050952615482,\n",
       " 0.04294773313203748,\n",
       " 0.04311273786394546,\n",
       " 0.04306581275767251,\n",
       " 0.043317002735125856,\n",
       " 0.043679746699829894,\n",
       " 0.04342239123376969,\n",
       " 0.043283999244404306,\n",
       " 0.04321857225388671,\n",
       " 0.04305597907521679,\n",
       " 0.04274939046008512,\n",
       " 0.04296783545272954,\n",
       " 0.04394089797420836,\n",
       " 0.04578878078609705,\n",
       " 0.04600897044431241,\n",
       " 0.04562841327313115,\n",
       " 0.047376044679346474,\n",
       " 0.049540012453993164,\n",
       " 0.05050600089386783,\n",
       " 0.05019336547493265,\n",
       " 0.05003709199113978,\n",
       " 0.05002599727403331,\n",
       " 0.0505361524005623,\n",
       " 0.05034433639738509,\n",
       " 0.05028075496963364,\n",
       " 0.051152111138952405,\n",
       " 0.05094381524637962,\n",
       " 0.05089597244622167,\n",
       " 0.0509638586556729,\n",
       " 0.050559522433563917,\n",
       " 0.0501896773185581,\n",
       " 0.05052977644534099,\n",
       " 0.050219139769015944,\n",
       " 0.05082837158578982,\n",
       " 0.05223480661292202,\n",
       " 0.052347697521604244,\n",
       " 0.0526723645820792,\n",
       " 0.052321130540396006,\n",
       " 0.052401408527253404,\n",
       " 0.05230226382237235,\n",
       " 0.05241672031750733,\n",
       " 0.05259462708590536,\n",
       " 0.052464073399148346,\n",
       " 0.05289651718056571,\n",
       " 0.05257640616445426,\n",
       " 0.0523919398450981,\n",
       " 0.051997116682569275,\n",
       " 0.051740688100680075,\n",
       " 0.05162548762746155,\n",
       " 0.052997198264983274,\n",
       " 0.05304391365110253,\n",
       " 0.05323306858647338,\n",
       " 0.05470657592532454,\n",
       " 0.05440058984331847,\n",
       " 0.054045061644677435,\n",
       " 0.054162656638771296,\n",
       " 0.05416124370614333,\n",
       " 0.05440335512879913,\n",
       " 0.05461039484725916,\n",
       " 0.05435556754137772,\n",
       " 0.05442072465800895,\n",
       " 0.05428512883905805,\n",
       " 0.05418741347790329,\n",
       " 0.0539922985962515,\n",
       " 0.05399994385678933,\n",
       " 0.05389977760130057,\n",
       " 0.054022499911404925,\n",
       " 0.05380401127108366,\n",
       " 0.053743984742576016,\n",
       " 0.053680706477770915,\n",
       " 0.05364760945750666,\n",
       " 0.053605909072893095,\n",
       " 0.0533147077043344,\n",
       " 0.05343599372927647,\n",
       " 0.053180875609137326,\n",
       " 0.053142288317582734,\n",
       " 0.05335596412075811,\n",
       " 0.053153819996914285,\n",
       " 0.052895920014487126,\n",
       " 0.0533398913513344,\n",
       " 0.0533704194581757,\n",
       " 0.05338947878632423,\n",
       " 0.05309641740811793,\n",
       " 0.052977043961850455,\n",
       " 0.052829532917322855,\n",
       " 0.05261038048793712,\n",
       " 0.05253484785162772,\n",
       " 0.052576648507409604,\n",
       " 0.052443218379151786,\n",
       " 0.05224095494339957,\n",
       " 0.05218906885420438,\n",
       " 0.05223242094832825,\n",
       " 0.05199889065755278,\n",
       " 0.05198859331695123,\n",
       " 0.05176720690697704,\n",
       " 0.051648350869954535,\n",
       " 0.05167514258288745,\n",
       " 0.051883847611638424,\n",
       " 0.052026021535441815,\n",
       " 0.052229703505958854,\n",
       " 0.0521302852752235,\n",
       " 0.05209357417542596,\n",
       " 0.05187768010736638,\n",
       " 0.05176069779386152,\n",
       " 0.05161079430642227,\n",
       " 0.05185854724741408,\n",
       " 0.05173702345663597,\n",
       " 0.05266326449358952,\n",
       " 0.05264950577389407,\n",
       " 0.05248534501733144,\n",
       " 0.052369886194355786,\n",
       " 0.052532071085615875,\n",
       " 0.05237550584466329,\n",
       " 0.05229968049619449,\n",
       " 0.052316525862450995,\n",
       " 0.052433547370035104,\n",
       " 0.05228042903216055,\n",
       " 0.05241302241705716,\n",
       " 0.052501452217334287,\n",
       " 0.05240781648853232,\n",
       " 0.052395251964365966,\n",
       " 0.05233522274663077,\n",
       " 0.052160545322597805,\n",
       " 0.052334917441415815,\n",
       " 0.05239946797691746,\n",
       " 0.052571966740278864,\n",
       " 0.05260814605898471,\n",
       " 0.052531905351726686,\n",
       " 0.05235169426481606,\n",
       " 0.0521797508530169,\n",
       " 0.052019488208461556,\n",
       " 0.05189245184234793,\n",
       " 0.05200148976205083,\n",
       " 0.05203653368502488,\n",
       " 0.05192789730533738,\n",
       " 0.05176691602143209,\n",
       " 0.051557838344874166,\n",
       " 0.05138736038023363,\n",
       " 0.05156950713269627,\n",
       " 0.051435609831603946,\n",
       " 0.05124981533514247,\n",
       " 0.05140338349707813,\n",
       " 0.0511927219801846,\n",
       " 0.05113639946187466,\n",
       " 0.05119161618977924,\n",
       " 0.05105246983381898,\n",
       " 0.051251584651052125,\n",
       " 0.05109097573837514,\n",
       " 0.05098593367024436,\n",
       " 0.0509000973112575,\n",
       " 0.05079007578421046,\n",
       " 0.05068039478169442,\n",
       " 0.050540387080898426,\n",
       " 0.0504579406958324,\n",
       " 0.0507024420034473,\n",
       " 0.05068037073231406,\n",
       " 0.05075767152093812,\n",
       " 0.05070391634175562,\n",
       " 0.050743068931951074,\n",
       " 0.0506893320936368,\n",
       " 0.050548056590006406,\n",
       " 0.05041212780054385,\n",
       " 0.050453457174469425,\n",
       " 0.05047606869876513,\n",
       " 0.05068190209169546,\n",
       " 0.05057980337754843,\n",
       " 0.05049209651474099,\n",
       " 0.050364139654874046,\n",
       " 0.05022639320662417,\n",
       " 0.05008853361337878,\n",
       " 0.04999951919307932,\n",
       " 0.04989283192995549,\n",
       " 0.049734416167628914,\n",
       " 0.04964552763389585,\n",
       " 0.04991775939286854,\n",
       " 0.04980187266684916,\n",
       " 0.049672283697873354,\n",
       " 0.0500694517958683,\n",
       " 0.05005620619387276,\n",
       " 0.04994076908188292,\n",
       " 0.04981658547744155,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test/validation set: Average loss: 0.0022, Accuracy: 9624/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss = test_model(model, test_loader, loss_fn)\n",
    "# Save the model\n",
    "save_model('data/conv.pt', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Familiarize yourself with the code above and consult the PyTorch documentation when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Experiment with different NN architectures (also varying the convolutional parameters: size, stride, padding, etc) and observe the effect wrt. the loss/accuracy on the training and validation dataset (training, validation). Note that when adding new layers (including dropout [Lecture 2, Slide 13], pooling, etc.) you need to make sure that the dimensionality of the layers match up. **IMPORTANT:** ignore the test set at this stage (i.e., comment out the relevant lines above) so that the results for the test set do not influence your model choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In the model above we use a simple gradient descent learning scheme. Try other types of optimizers (see https://pytorch.org/docs/stable/optim.html) and analyze the effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lastly, save your best model and results. At the next lecture we will then see who got the best results :-) Note that for this to be meaningful it is important that you have not relied on the test set while doing model learning/selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. If you feel adventurous, try investigating some of the other datasets that come prepacakged with PyTorch (see https://pytorch.org/vision/0.8/datasets.html). For instnce, for FashionMNIST you only need to change the dataloader from datasets.MNIST to datasets.FashionMNIST."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a52c312340d997e0f2d76c4a69880a6c0d3df73b0efc89e90ec38cf5ac1fd14a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
