{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self study 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this self study we investigate some community detection (graph clustering) techniques.  We make extensive use of the Networkx package. The documentation can be found here: https://networkx.org/documentation/stable/reference/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import sklearn as skl\n",
    "\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a social network consisting of 71 lawyers. A description of the network and the original data can be found here:\n",
    "\n",
    "https://www.stats.ox.ac.uk/~snijders/siena/Lazega_lawyers_data.htm\n",
    "\n",
    "Of the three different relationships included in the data we will only be using the 'friendship' relation. This is a directed relationship, i.e., friends(a,b) does not necessarily imply friends(b,a) according to the data.\n",
    "\n",
    "We load a version of the Lazega network data that only contains the 'friends' edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazega=nx.readwrite.graphml.read_graphml('lazega.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes in the graph have the attributes \"Practice\" \"Age\" \"Seniority\" \"Office\" \"Gender\" \"Status\" . To obtain a dictionary with the values for a specified attribute for all nodes, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.get_node_attributes(lazega,'Office')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a little helper function that returns an array of attribute values of nodes according to the order in which nodes are returned by the G.nodes() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_array(G,att_name):\n",
    "    ret_array=np.zeros(nx.number_of_nodes(G))\n",
    "    for i,n in enumerate(G.nodes()):\n",
    "        ret_array[i]=G.nodes[n][att_name]\n",
    "    return(ret_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to plot the graph using one of the layout algorithms provided by networkx, and nodes colored according to one of the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(lazega,with_labels=True,node_color=get_att_array(lazega,'Office'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a bit simpler, we turn the directed graph into an undirected one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazega=lazega.to_undirected()\n",
    "nx.draw_kamada_kawai(lazega,with_labels=True,node_color=get_att_array(lazega,'Office'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networkx.algorithms.community.quality.modularity function can be used to measure the similarity score of a graph clustering. The clusters (communities) have to be provided as a list of lists of nodes. The following code provides the necessary transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = 'Office'\n",
    "att_array = get_att_array(lazega,attribute)\n",
    "att_dict= nx.get_node_attributes(lazega,attribute)\n",
    "att_communities = []\n",
    "for c in np.unique(att_array):\n",
    "    comm_c =[]\n",
    "    for n in lazega.nodes():\n",
    "        if att_dict[n]==c:\n",
    "            comm_c.append(n)\n",
    "    att_communities.append(comm_c)\n",
    "    \n",
    "print(\"Modularity score of the communities defined by attribute ''{}'': {}\".format(attribute,modularity(lazega,att_communities)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Use the networkx implementations of the Newman-Girvan algorithm to divide the Lazega network into 2,3,4,5 communities. Compare the communities returned by the algorithms with the communities defined by the attributes:\n",
    "    \n",
    "To what extend do the detected communities recover the attribute-defined communities? This is a case of what is called supervised cluster evaluation: a class label (or attribute) not used in the clustering process is assumed to represent a true underlying clustering, and we want our clustering method to recreate these underlying \"ground truth\" clusterings as much as possible (even though we cannot be sure that these \"ground truth clusters\" are indeed the most relevant clusters -- especially here, where we have multiple attributes that define competing \"ground truths\").\n",
    "        \n",
    "Compare the different community structures (attribute-based, and Newman-Girvan) according to their modularity scores.        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to the model-based community detection method. A \"cheap\" way to obtain embeddings of the nodes into 2-dimensional space is to use a graph layout algorithm. Networkx provides a few layout algorithms that return a dictionary of nodes:coordinates :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=nx.kamada_kawai_layout(lazega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Calculate the log-likelihood of the lazega graph based on embeddings provided by different layout algorithms (cf. slide 31). Which layout provides the best \"explanation\" for the observed edges based on our probabilistic model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given coordinates for the nodes, we can now fit a Gaussian mixture model to these points. First we need to reshape our data again a little bit to fit the requirements of the next method in our pipeline: we turn the dictionary of positions into a standard array, where the first column contains the node identifier (not really needed in the following, but good to have it for possible checks along the road):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_array=np.zeros((len(embedding),3))\n",
    "\n",
    "for i,k in enumerate(embedding.keys()):\n",
    "    pos_array[i,0]=int(k)\n",
    "    pos_array[i,1:3]=embedding[k]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sklearn.mixture.GaussianMixture class. Apart from specifying the number of mixture components, we can also restrict the covariance matrices in different ways. For example, covariance_type='tied' means that in the learned model all components have the same covariance matrix (which is the same restriction that we saw in the LDA classification model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm=GaussianMixture(n_components=2,covariance_type='tied').fit(pos_array[:,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing clustering, we are using the 'predict' function of the learned mixture models on the training points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=gmm.predict(pos_array[:,1:3])\n",
    "\n",
    "nx.draw(lazega,pos=embedding,with_labels=True,node_color=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Compare the modularity scores obtained by Gaussian mixture models with n_components = 2,3,4,5 and embeddings obtained from different layout methods. How does the modularity score relate to the likelihood scores computed in Task 2 -- do embeddings with higher likelihood score lead to clusterings with higher modularity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using coordinates calculated by layout algorithms for our graph mixture model is of course not what one is supposed to do. The \"proper\" approach is to learn vectors z_i by maximizing the log-likelihood function. A full implementation of such an approach is outside the scope of what we can do in this self study. However, with relatively little effort we can take some steps in this direction. \n",
    "\n",
    "We make use of the fact that we already have a very special vector representation of the nodes in form of the rows of the adjacency matrix, which we can access as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am=nx.adjacency_matrix(lazega)\n",
    "idx=2\n",
    "print(\"The adjacency vector for node {} in a sparse vector representation is \\n{}\".format(list(lazega.nodes)[idx],am[idx,:]))\n",
    "print(\"The same in a standard dense vector representation: \\n{}\".format(am[idx,:].todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dense vector representations, we can use the np.linalg.norm function to compute the Euclidean distance between two adjacency vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx0=0\n",
    "idx1=3\n",
    "print(\"The distance between the adjacency vectors of nodes {} and {} is {}\".format(list(lazega.nodes)[idx0],\\\n",
    "                                                                              list(lazega.nodes)[idx1],\\\n",
    "                                                                                  np.linalg.norm(am[idx0,:].todense()-am[idx1,:].todense())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Construct the 69x69 distance matrix between the nodes of the lazega graph based on the Euclidean distance between the adjacency vectors. Use this matrix and the multidimensional scaling implementation sklearn.manifold.MDS to construct 2-dimensional vector representations for the nodes. Fit again a Gaussian mixture model to these new representations, and compare the modularity score you get now to what you got in the earlier tasks. With this approach we are not limited to 2-dimensional vector representations: how do things change when you use multidimensional scaling with a target dimension of 3 or 4 instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
